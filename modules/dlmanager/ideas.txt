Hmm, let's write a new python module. This module will have a CLI for users. The purpose of this module is to encapsulate every single different method of copying and uploading and moving out there as I find myself doing a variety of things SCP are sync are sync to a Windows machine that's using CYG win so the path things different, are clone, Google drive etc.

Basically it should be like a download manager or maybe a transfer manager..

The module is run, and once running (can be run without any transfers), transfers can be added (which the manager handles by spinning off new instances of copier script processes (for example the manager is running idle , the user want to initiate a copy of some files from current machine Termux to a location on a W11 machine. Automatic mode, specified by an argument or by default on (unless a specific transfer program is specified via an argument), Will walk through the different available transfer methods starting with the most the best and ending with the least optimal so probably starting with our sink and then trying our clone and then trying SCP etc..

This program should handle things like, when trying to initiate an rsync transfer between devices, from a Linux device to a Windows device, ensure that the cygwin drive/path syntax is used if needed, instead of immediately giving up on using rsync in the transfer because the provided path format was wrong..
It should handle transfers mainly automatically from one device to another will also allowing high levels of customization if desired so initiating a transfer between two devices should try the most optimal transfer method first starting as a process if that doesn't work try the next one etc etc.
The user should provide at a minimum the source to copy, and the destination (it same machine just a path - relative or absolute is adequate, otherwise require a user@IP_or_named_address). In addition to the destination machine, the user needs to provide the destination path (default to ~/).
Have args that enable replacing files at destination with source files if conflicts, deleting source files after each finished file transfer, partial file transfers (restartable/pauseable/etc).

The dlmanager should, even if a transfer is in progress, be able to spin up as many new transfers as required (either via the TUI from the run screen of a GUI mode running dlmanager, or via running the module CLI and running a new transfer command). If there's an existing dlmanager running in TUI mode, any new transfers started by CLI should be added to the tracked transfers, showing real time -in-place stats updates of each unique transfer.

We can structure the module like this they'll be a single download manager Python script file, And then each downloader program will get its own script file (rsync-dler.py, rclone-dl.py, scp-dl.py, etc ). This way all the customizations related to that specific downloader type or transfer type program can be maintained in its own file and the manager can just spin up a new process by just running a new instance of a file to start a new transfer.

Each of the downloader files should follow the paradigm I implemented for my ytaedl module. Which had it download manager script and then had a worker downloader.py file. Downloader.pi communicated to the download manager via outputting a long string of json every update, which the dlmanager script watches and ingests for each dl-worker script it starts.. I'm not sure exactly how to handle an existing dlmanager.py this running, then somewhere else a transfer is spun up via a new CLI call.. we need to make sure the new CLI call ends up with that new transfer script being started by the already running dlmanager instance, or at the very least the running dlmanager instance needs to start watching the output of the newly spun up dl-worker .

The worker outputs all statistics about the transfer it's working on that are available via the program it's running. So our sink should be run in a manner in which it is most verbose, And then within the RSync worker script should be co-dedicated to watching and parsing the output of a verbose rsync call, that info should then be isolated, and packages up into a nice json string something like:
{bytes-dl: xxx, total-bytes: xxx, bytes-per-s: 123, current-filename: "filename and path", cur-file-bytes-dl: xxx, cur-file-total-bytes xxx, file-number: x, total_file_count: xxx,}
Etc. etc. etc. This Jason string will be printed by the worker either every time the parser that's parsing the output of the download program parse is an update or a certain number of times a second. Each downloader script will be specialized to parse its own program output and each download worker will also have its own log file, where things are logged in the same syntax or similar to that of ytaedl.
This idea of how to pass information between the spawned processes and the download manager or transfer manager is just an idea and something that worked with the other module ytaedl..
