Advanced Techniques for Optimizing vdedup Video Deduplication

High-Recall Detection of Near-Duplicate Videos (Including Partial Overlaps)

Achieving high recall in video deduplication requires techniques that can catch not only exact duplicates but also near-duplicates and partial overlaps. This means the system must recognize when two videos share content even if one is embedded partially (≥10% overlap) in the other, or if they differ in resolution, encoding, or minor edits. To maximize recall:

Robust Video Fingerprints: Use perceptual video hashing or fingerprinting methods that are invariant to common transformations (resizing, re-encoding, cropping, watermarks, frame rate changes, etc.). For example, a wavelet-based video hash like VideoHash generates a 64-bit hash that remains stable despite scaling, transcoding, or minor edits. This provides a quick way to flag obvious duplicates. However, note that such one-pass hashes typically treat the video as a whole and may not detect a case where one video is just a clip of another. We can use these global hashes for an initial fast scan, but additional steps are needed for partial matches.

Sequence-Based Matching for Partials: Incorporate algorithms that compare sequences of frames rather than whole-video signatures. Instead of treating each video as one monolithic entity, break them into a sequence of content fingerprints. Research shows that comparing entire videos at once can miss overlaps of short segments; it's crucial to partition videos into smaller segments for comparison. We can generate a fingerprint for each scene or each fixed-length segment and then use sequence alignment techniques to find overlapping subsequences. For instance, one approach is to use a form of Dynamic Time Warping (DTW) or sequence alignment to allow a shorter video to slide along a longer one until a region of high similarity is found. Another approach from prior systems (e.g. ViDeDup) is to compute frame-level ordinal signatures and compare videos segment-by-segment to catch inserted or chopped clips. In practice, this could mean detecting a run of N consecutive frames in Video A that closely match N frames in Video B (within some tolerance), indicating a potential partial duplicate. By detecting contiguous matching segments (not just isolated similar frames), we can reliably catch embedded clips or overlaps of ≥10% content.

Multiple Modalities for Recall: Use multi-modal fingerprints to cast a wider net. In addition to visual content, generate an audio fingerprint for each video (if audio is present). Audio-based matching can discover duplicates even when visuals are modified (or vice versa). Tools like Chromaprint (from the AcoustID project) produce compact fingerprints designed to identify near-identical audio content. By comparing audio fingerprints, the system can catch cases where the same soundtrack or dialogue indicates duplicate content, even if the video frames were altered. This is especially useful for partial overlaps: a common audio segment can signal a duplicate scene. Audio matching has high precision for identical audio and can tolerate re-encoding or slight quality changes in the audio stream. Integrating audio fingerprint checks alongside visual checks will improve recall for cases where one of the streams (audio or video) is unchanged between duplicates.


In summary, to achieve high recall, vdedup should combine robust perceptual hashing, sequence-based segment comparison, and multi-modal (audio/visual) fingerprinting. The goal is to ensure that even a small common segment (10% or more of content) triggers a similarity match. By using tolerant matching (perceptual hashes or deep features) and by examining videos in segments rather than only as wholes, the system will catch near-duplicates that differ in format or contain partial content overlaps.

Scalability for Large Datasets (~6TB of Video)

Handling thousands of videos (on the order of 6 TB) demands an architecture that is efficient in both computation and memory. Key considerations for scalability include efficient frame processing, indexing for similarity search, and minimizing redundant work:

Efficient Frame Extraction: Rather than decoding every frame of every video (which would be prohibitively slow for 6 TB of data), extract a smart subset of frames. Use FFmpeg for accelerated video decoding and frame capture, possibly with hardware support. For example, FFmpeg can sample frames at a fixed interval or scene changes; a command like ffmpeg -i input.mp4 -r 0.1 frames_%02d.jpg will grab 1 frame every 10 seconds. vdedup can dynamically adjust the sampling rate based on video length: longer videos might be sampled at lower frame rate (to limit total frames), while ensuring a minimum number of samples for short clips. If a GPU is available, leverage FFmpeg’s NVDEC/CUDA for fast decoding (as the codebase already hints with gpu=True options) so that even high-resolution 4K videos can be decoded and sampled quickly. Frame extraction can be parallelized across multiple CPU cores or machines – e.g. processing different videos in parallel – to speed up throughput, given sufficient I/O bandwidth.

Caching and Reuse: For a one-time scan of 6 TB, decoding is heavy, so vdedup should cache intermediate results (frame fingerprints, features, etc.) to disk. If the system already writes out JSONL or binary cache files (e.g. vdedup-cache.jsonl as seen in the repo), expanding this to store new feature types is crucial. This way, subsequent runs or incremental updates only process new or changed videos. The cache might contain pHashes, audio fingerprints, and deep feature vectors for frames. Considering storage trade-offs: storing every frame’s data for thousands of videos might be large, so caching at a sensible granularity (like one feature per second or per scene) is a good compromise.

Indexing for Similarity Search: A naive all-vs-all comparison of frames or even per-video hashes across thousands of videos is expensive. Introduce an indexing scheme to handle similarity queries at scale. One approach is using FAISS (Facebook AI’s similarity search library) or an open-source vector database like Milvus for indexing high-dimensional feature vectors. For example, after extracting feature embeddings for key frames (using a deep model as described later), insert all these vectors into a FAISS index for approximate nearest neighbor search. FAISS can handle millions of vectors efficiently in-memory or on-disk, and it supports various indexing strategies (IVF, HNSW, PQ) to balance speed/accuracy. This allows us to query, for each frame or segment of a new video, the most similar frames in the entire dataset in sub-linear time. Vector indexing will be much faster than brute-force comparison, enabling the system to scale to large datasets without exploding in comparison cost. If memory is a concern (millions of feature vectors), consider using a coarse quantizer or limiting to a representative subset of frames per video for the index.

Incremental and Distributed Processing: Design the pipeline to handle the dataset in chunks. For example, process videos batch by batch (compute fingerprints, update index) to avoid needing all data loaded at once. The architecture could allow distributing the load: one machine (or process) extracts frames, another computes features, another handles indexing and comparison. The use of open tools like FFmpeg, OpenCV, and PyTorch means we can take advantage of multiple threads and even GPU acceleration for the heavy parts (decoding and neural network inference). This concurrency is essential when dealing with many terabytes of video.

Memory and I/O Optimization: 6 TB of video data implies a lot of disk I/O. Use streaming and pipeline processing to avoid intermediate bloat: e.g., pipe frames directly from FFmpeg into memory (or a CPU/GPU buffer) for hashing to avoid writing thousands of images to disk. When using deep learning models or computing hashes, batch operations where possible (process multiple frames together) to utilize vectorized operations on modern hardware. Also consider using lower precision data types for embeddings (e.g., FP16 or INT8 quantization for neural features) to reduce memory and speed up similarity computations, with minimal impact on recall.


In short, to handle large datasets, optimize each stage for scale – sample frames efficiently, reuse computations via caching, and employ scalable search structures (like FAISS or LSH indices) to avoid quadratic blowup. Combining these strategies will let vdedup maintain high throughput even as the video corpus grows into the thousands.

Configurable Trade-offs Between Speed and Accuracy

Different use cases may favor speed over thoroughness or vice versa. vdedup can be enhanced with configurable modes that adjust the depth of analysis, allowing users to dial up accuracy at the cost of speed or choose a faster, lighter scan when needed. We propose three modes, with a balanced mode as default:

Fast Mode (Speed-Optimized): This mode prioritizes minimal processing. It would use coarse but quick techniques to catch obvious duplicates. For example, in fast mode vdedup might compute only a single fingerprint per video (such as the perceptual video hash or a few evenly spaced frame hashes) and compare those. Tools like VideoHash (64-bit hash per video) are extremely fast to compute and compare. Fast mode could skip partial overlap detection and focus only on full-video similarity. It might also use aggressive indexing (with higher tolerance for false negatives) to reduce comparisons. The result is a rapid scan that might miss some near-duplicates but can process the 6 TB dataset quickly to find most exact duplicates or very high-similarity videos.

Balanced Mode (Default): The default mode should blend speed and accuracy for everyday use. This mode can enable multi-stage processing: first do a quick coarse pass (like the fast mode techniques) to eliminate completely dissimilar videos, then perform a finer analysis on likely matches. For instance, balanced mode might sample a moderate number of frames per video (e.g. one frame every X seconds or per scene) and use perceptual hashes for an initial similarity check. Potential matches from this stage (above a similarity threshold) would then be verified with more accurate methods (like deeper frame-by-frame comparison or SSIM). Balanced mode would also include partial overlap detection for a reasonable overlap threshold (e.g. it will catch if 20% of content overlaps, but perhaps not chase extremely small overlaps to save time). The idea is to catch nearly all true duplicates/overlaps while keeping runtime reasonable – e.g. by limiting the number of pairwise comparisons through good candidate pruning.

Thorough Mode (Accuracy-Optimized): This mode pulls out all the stops for maximum detection, suitable for one-time deep cleaning or forensic uses. Thorough mode would use all available techniques in combination: denser frame sampling (e.g. every second or at every scene change), deep learning embeddings and nearest-neighbor search for visual similarity, audio fingerprint matching, and exhaustive subsequence search for partial duplicates. It may lower thresholds to catch even 5-10% overlaps. To manage the increased load, thorough mode can still leverage indexing and multi-stage workflow, but with more exhaustive parameters – for example, retrieving a larger Top-K from the FAISS index for each query to ensure no match is missed. It might also double-check borderline cases using computationally expensive metrics like SSIM on many frame pairs to confirm. This mode sacrifices speed (it might be several times slower than balanced mode) but yields the highest recall and confidence that no duplicates were overlooked.


Implementing these modes would involve exposing configuration options (CLI flags or config file) for things like frame sample rate, which algorithms to use (pHash vs. deep vs. audio), and similarity thresholds. For example, a --fast flag might set pHash sampling to 1 frame per 20 seconds and skip audio analysis, while --thorough might set 1 frame per 1 second plus enable neural embeddings and audio checks. Internally, the pipeline can be designed to toggle components based on these settings (e.g., skip the deep feature stage if not needed). By default, balanced mode provides sensible middle-ground parameters – perhaps using pHash on, say, 10 frames per video, plus a quick deep feature check on a couple of keyframes – which gives robust detection in most cases without heavy resource usage.

Providing these configurable trade-offs ensures vdedup is flexible: a user can do a quick scan of a massive library in minutes, or a deep analysis when accuracy is paramount, all with the same tool. The code should be modular to make it easy to turn features on/off according to the selected mode.

Integration of Open-Source Tools and Libraries

To implement the above capabilities, vdedup should leverage powerful open-source libraries rather than reinventing the wheel. Key integrations include:

FFmpeg (and ffprobe): FFmpeg is essential for video decoding, frame extraction, and format handling. vdedup can use FFmpeg to read video files and grab frames at specified intervals or keyframes. For example, using FFmpeg’s CLI or libav bindings, vdedup can seek to timestamps or scene boundaries and export frames to memory for analysis. ffprobe can be used to quickly get metadata (duration, resolution, codec) for heuristics or to allocate resources. FFmpeg's broad codec support ensures vdedup can handle anything from 240p AVIs to 4K MP4s without separate parsing code. Additionally, FFmpeg’s filters could be utilized for preprocessing, e.g. scaling frames to a standard size, or even for generating audio fingerprints via the astats or afftdn filters – though using Chromaprint might be simpler. Using FFmpeg with multithreading or even GPU acceleration (through NVDEC/NVENC) will significantly speed up the heavy I/O of reading video files.

OpenCV: OpenCV provides a convenient interface for image operations on frames. vdedup can use OpenCV (cv2 in Python) to convert FFmpeg-extracted frames into matrices, perform resizing, conversion to grayscale, etc., as needed for hashing algorithms. OpenCV also has an img_hash module which includes implementations of perceptual hashes like pHash, average hash, etc., which might be utilized as an alternative to the Python imagehash library. Additionally, OpenCV can compute structural similarity (SSIM) between images – a metric we plan to use for refined comparisons. If integrated in C++ or via python bindings, OpenCV could compare frames with SSIM to validate a match (SSIM gives a similarity score 0-1). OpenCV is also useful for any feature-based methods (like ORB or SIFT keypoints matching between frames), though these are heavier; deep learning features likely supersede them in our design. Another use of OpenCV could be scene change detection (to drive keyframe selection) using methods like color histogram differences; however, specialized libraries like PySceneDetect can also be used for scene detection if needed.

PyTorch (Deep Learning Models): For deep visual hashing and embeddings, integrate PyTorch to use pre-trained CNN models. For example, a ResNet50 or EfficientNet model pre-trained on ImageNet can serve as a feature extractor: feed it a frame and take the activation from a late layer (e.g., the pool5 layer of ResNet50 yields a 2048-D feature). These feature vectors capture visual content in a robust way. Using PyTorch, vdedup can load a model in evaluation mode and process frames in batches on the GPU to get embeddings quickly. Another option is to use specialized video models (like a 3D CNN or SlowFast network), but those are heavier; a frame-based approach is usually sufficient for duplicates. PyTorch also allows using models like VGG16 or ResNet152 as mentioned in literature – outputting 1000-D classification vector or better, a truncated 4096-D feature before the final classifier for more content information. The integration of PyTorch will enable the deep hashing stage: we could even fine-tune a model on known duplicate vs non-duplicate pairs if we have labeled data, but even off-the-shelf embeddings combined with a similarity search give great results in practice.

FAISS (Facebook AI Similarity Search): As mentioned, FAISS is ideal for handling large-scale vector comparisons. vdedup can use FAISS to build an index of all frame feature vectors (or all video-level embeddings). Then, finding potential matches for a given video becomes a matter of querying the index for nearest neighbors. For example, for each key frame of a new video, retrieve the top 5 most similar frames in the database. If many of those come from the same candidate video, that’s a strong sign of duplication. FAISS supports Hamming indexes too, so even if we use binary hashes (like 64-bit videohash or pHash), we can index them for Hamming-distance search. Alternatively, for set-based similarity (Jaccard), we might integrate a MinHash library (such as Python’s datasketch for MinHash and LSH) to index video frame hashes for quick subset comparisons. The overall benefit of FAISS and similar libraries is that they are optimized in C++ with vector instructions, and can handle millions of vectors – crucial for scaling to 6TB of video data. Integrating FAISS might involve some up-front index building time, but querying will be very fast, enabling interactive or on-the-fly duplicate searches even in a large corpus.

Chromaprint/AcoustID (Audio fingerprinting): For audio, integrate Chromaprint’s library (it has Python bindings and a command-line tool fpcalc). Chromaprint generates a compact fingerprint (e.g., 32 byte code) for an audio track which can be compared for similarity. By running Chromaprint on each video's audio (or on segments of audio), vdedup can identify videos with the same or very similar audio. The library is optimized for speed and designed for exactly this purpose: duplicate audio detection and long stream monitoring. We could use it in two ways: (1) compute a single fingerprint per video (Chromaprint by default uses the first 120s or full audio to make one fingerprint) and compare those for overall audio similarity, and (2) for partial overlap, slide through the audio in chunks (say 30-second windows) computing fingerprints to find if any chunk of audio in one video appears in another. Chromaprint is robust to minor differences in encoding and even some background noise, focusing on the core audio patterns. The integration can be done by calling fpcalc and reading its output or using the chromaprint Python API for direct fingerprint generation.

Metadata and File Utilities: Use simple libraries or built-ins for metadata. For example, mediainfo or FFmpeg can fetch metadata like duration, resolution, bitrate which we can use in heuristics (like preferring higher bitrate in deletions). Python’s file libraries can be used to do quick MD5/SHA1 checks for exact file duplicates (as a preliminary step, if two files are bitwise identical we can mark them dup immediately). While these are not advanced algorithms, integrating such basic checks and metadata analysis (perhaps via a library like pymediainfo or FFmpeg’s JSON output) will make the overall system more robust and user-friendly.


All these open-source tools are free and widely used, meaning we benefit from their reliability and performance. The vdedup architecture should call out to these libraries at appropriate stages (e.g., FFmpeg for decoding, then OpenCV/PyTorch for image processing, FAISS for search, etc.), passing data through a pipeline. By building on these components, we avoid writing low-level code from scratch and ensure that vdedup aligns with current best practices in video processing and similarity search.

Perceptual, Structural, and Deep Hashing; Audio Fingerprinting; Metadata Heuristics

This section outlines how to combine multiple techniques to identify duplicates, each contributing to either speed or accuracy, and how to use metadata to refine results:

Perceptual Visual Hashing: Perceptual hashes (pHash, aHash, dHash, etc.) convert an image/frame into a compact fingerprint that looks similar for visually similar inputs. vdedup already uses pHash (perceptual hash) for frames, which is a good start. We recommend continuing and extending this: compute a pHash for a set of key frames from each video. pHash works by taking a DCT of the image and encoding low-frequency components, making it robust to resizing and minor quality changes. Two identical scenes at different resolutions will yield very close pHash values (Hamming distance will be low). By comparing frame hashes, we can quickly flag videos that likely share content. However, to catch partial overlaps, we need to hash multiple frames per video. Strategies include hashing scene boundaries (so that each distinct scene yields a hash) or hashing at a fixed time interval (e.g., one every X seconds). Increasing the number of pHash samples per video improves chances of finding overlaps, at the cost of more comparisons. To keep it efficient, we can use a hash index or LSH: e.g., treat each 64-bit pHash as a point in Hamming space and use an LSH table to find near-matches in O(1) time. This avoids comparing every hash to every other hash directly. Perceptual hashes are fast to compute (ms per frame) and very compact, so they form the first line of defense for both speed and memory efficiency. We should also consider variants of pHash or complementary hashes: e.g., color histograms for scenes (to capture color-based similarities), or frame-average fingerprint (like average hash which is even simpler than pHash). By combining a couple of different hashes per frame, we reduce the chance of missing a match due to an edge case in one hash algorithm.

Structural Similarity (SSIM) for Validation: SSIM is a metric that compares two images in terms of luminance, contrast, and structure – essentially giving a similarity score based on human perceptual similarity. Using SSIM directly on every frame pair would be too slow, but it’s extremely useful for verification and for reducing false positives. The open-source tool Vidupe uses pHash to find candidates and then SSIM to confirm matches. We propose a similar two-stage approach: use fast hashing or embedding to propose that video A and B might match, then for those pairs, decode a few representative frames from each and compute SSIM (or a related measure like PSNR) to ensure they truly are near-duplicates. SSIM can also be used in partial overlap: if we suspect a certain time range overlaps, compute the average SSIM over that overlapping segment’s frames to quantify the similarity. SSIM is more precise than pHash in that it will near 1.0 only if the frames are virtually identical (allowing for slight blurriness or encoding changes). It helps filter out false positives (e.g., two different videos that by chance had similar pHash for a frame due to a simplistic scene). Because SSIM is computationally heavier, we restrict it to the verification mode or to refine top candidates. The combination of pHash + SSIM is proven in other open-source duplicate finders (Vidupe and Video Simili Duplicate Cleaner) to be effective – pHash casts a wide net, SSIM tightens it.

Deep Visual Embeddings (Deep Hashing): Deep learning-based hashing or embedding is a powerful technique to capture video content. Rather than a handcrafted hash, we use a CNN to produce a high-dimensional vector that represents the frame’s content. For example, pass a frame through ResNet50 and take the 2048-D output of the global average pool layer as its feature. Two frames from the same scene (even if one is 4K and one 480p, or one has a logo) will yield feature vectors that are very close in Euclidean or cosine distance. By contrast, unrelated frames will be far apart. We can compress these vectors (via PCA or even hashing them to binary codes) to around, say, 128 dimensions to store and compare efficiently. Another advanced idea is to use sequence embeddings: e.g., use a recurrent network or transformer to embed a sequence of frames to capture temporal information, but a simpler approach is to aggregate frame embeddings (e.g., average pooling of frame vectors to get a single vector for the whole video). Some open research even trains models specifically for video hashing (e.g., deep supervised video hashing networks) to produce a compact hash for each video. For practicality, we can start with frame-level embeddings + FAISS as described earlier. This deep visual similarity approach will handle cases that pHash might miss – e.g., if a video has been significantly altered in brightness or slight changes, a CNN might still recognize the scene whereas pHash bits could flip. It’s also more robust to added overlays or slight crop: a CNN trained on ImageNet will still pick up the main objects/scene. The downside is computational cost – but with a GPU and batch processing, we can feasibly embed thousands of frames per minute. We should use deep embeddings in balanced and thorough modes to increase accuracy. As an example, a pipeline might extract 5 keyframes from each video, compute ResNet features, and average them to get a video-level vector. Then do a cosine similarity search among all video vectors to group similar videos (this catches even loosely similar ones). For partial matches, per-frame or per-segment embeddings compared via nearest neighbor search is more appropriate (because a portion of video B might match part of A, even if overall averages differ).

Audio Fingerprinting: As discussed, audio content is a valuable signal. vdedup should incorporate an audio hashing stage using Chromaprint or a similar acoustic fingerprint. The approach: for each video’s audio track, generate a fingerprint (Chromaprint outputs a sequence of fingerprints or a summarized fingerprint for the whole track). Then compare fingerprints between videos. If two videos have a high audio fingerprint similarity score, they likely contain the same audio segment. Audio fingerprint matching is already used in industry for finding copies of songs/videos (e.g., ContentID systems) because it’s very robust – it can recognize a song even if there’s added noise or the recording is slightly muffled. In our context, if Video X is a duplicate of Video Y but maybe one has slight visual differences, a matching audio fingerprint will immediately identify them as duplicates with high confidence. It can work in tandem with visual matching: one video might have the same visuals but different music dubbed – in that case visual will catch it but audio won’t match, and vice versa. By using both, we cover more ground. Implementation-wise, Chromaprint’s output can be compared via Hamming distance or through the AcoustID service for known matches. Since we want open-source/offline, we’d likely directly compare the fingerprint vectors ourselves (Chromaprint yields a series of numbers representing spectral peaks over time). Also, as a heuristic: if two videos have the exact same length and audio hash, you almost certainly have a duplicate or only trivial differences in video track.

Metadata Heuristics: While content-based methods are the primary drivers, metadata can greatly assist in decision-making once potential duplicates are found. vdedup can use metadata in a heuristic or ranking capacity. For example:

Duration: If one video is exactly the same duration (to the millisecond) as another and passes a content similarity check, it’s likely a full duplicate. If one is a subset (e.g., video A is 600s, video B is 60s, and content overlap is 60s), then B might be fully contained in A. Knowing the durations helps interpret the result (we might label B as a subset of A).

Resolution & Bitrate: Often users want to keep the highest quality copy. If two files are duplicates in content, we can automatically decide which to prefer by comparing resolution, bitrate, or file size. Some tools (like Video Duplicate Finder) present these attributes so the user can choose. We can automate a “keep highest quality” policy: e.g., among duplicates, mark the lower resolution or lower bitrate ones as candidates for deletion. vdedup can allow configurable policy (keep highest resolution, or keep larger file if that presumably has less compression).

Format/Codec: If the same video exists in different codecs, the user might have a preference (maybe keep the MP4/H.264 version over an AVI, etc.). We can detect duplicates then simply inform the user of format differences to aid their choice.

Timestamps/Metadata: Creation or modification timestamps might indicate which is the original, though not always reliable. Still, if the user wants to keep the newest or oldest copy, that could be a heuristic (for example, in surveillance footage deduplication, maybe keep the earliest occurrence).

File Path or Name: We avoid using name to detect duplicates (not reliable), but once duplicates are found, filenames or folder paths might hint which one is in a “preferred” location (perhaps user has a sorted library and a downloads folder; duplicates in the unsorted downloads could be the ones to remove). This goes more into user policy, but the tool can present such info.



In implementation, after content matching identifies groups of duplicates, metadata heuristics come into play to rank which file is the best candidate to keep. For instance, Video Simili Duplicate Cleaner not only finds matches via pHash/SSIM, but also auto-selects the lower-quality version for deletion using criteria like resolution or file size. vdedup can do the same: compute a score for each file in a duplicate set based on metadata (e.g., higher score if higher resolution, longer duration, newer codec, etc.) and recommend deletion of the lower-scoring file. This makes the deduplication process smarter and reduces the manual effort on the user’s side.

By combining perceptual hashing, structural comparison (SSIM), deep learning embeddings, audio fingerprints, and metadata analysis, we cover all angles. Perceptual and deep hashes cast a wide net to catch any visual duplicates, audio fingerprints catch those with matching sound, and metadata helps interpret and act on the findings. This multi-pronged strategy is aligned with best practices: content-based duplicate detection augmented by context metadata for final decision-making.

Improving pHash Sampling and Partial-Subset Detection

The current vdedup approach uses pHash sampling (e.g., a few frames per video, possibly at scene boundaries). To avoid missing partial overlaps, we need to refine both how we sample frames and how we detect subset relationships:

Denser and Smarter Frame Sampling: If only a handful of frames are hashed per video, a short overlapping segment could be overlooked simply because none of the sampled frames fell into that segment. To fix this, increase the sampling frequency adaptively. One strategy is to sample proportional to video length: for example, take a frame every N seconds, where N is chosen such that we get, say, 50–100 frames for the longest videos and at least a minimum (maybe 5–10) for short videos. This ensures coverage. Additionally, use scene-change detection to capture unique content: if a video has long static scenes, grabbing just one frame per scene might miss slight variations, so consider capturing a frame at the middle of a long scene as well as the boundary. Another idea is to utilize video keyframes: keyframes (I-frames in codecs) often indicate scene changes. By extracting all I-frames (or every kth I-frame if too many), we get a set of representative frames. The threshold of 10% overlap means even a relatively short segment (e.g., a 30-second clip in a 5-minute video) should have a few frames sampled. If we sample at 1 frame per second, a 30s segment gives 30 frames – likely enough for detection. If performance is a concern, one can default to a coarser rate (say 1 fps) in balanced mode, but in thorough mode it might sample 2–5 fps or every frame. The key is to avoid huge gaps in the sampling timeline. Also, ensure both videos in a comparison have sufficiently aligned sampling: e.g., if one video’s scene wasn’t sampled at all because we hit a scene cut right before it, we might miss a match. Using a sliding window sampling (taking overlapping segments) can help: for instance, take a hash every 5 seconds with a 1-second offset on a second pass to cover intermediate points. These adjustments will significantly reduce the chance of missing overlaps.

Subset (Contained Video) Detection Algorithm: Beyond sampling, we need a robust way to detect when one video is a subsequence of another (or when two videos share a common subsequence). One effective approach is to treat each video’s frame-hash sequence like a DNA sequence and look for matches between sequences. We can implement a form of sequence alignment or matching subsequence search:

One simple technique: For each video, keep an ordered list of pHashes (or frame feature vectors). Then, for each frame hash in video A, search for near-identical hashes in video B (via a hash look-up or nearest-neighbor search). When a matching hash is found, check the neighboring hashes to see how long the match runs consecutively. If we find a run of, say, M successive frames in A that match (within tolerance) M successive frames in B, then we’ve found an overlapping segment. If that segment length or proportion exceeds the 10% threshold, flag the videos as partial duplicates.

We can optimize this by first mapping out all frame matches between A and B using an index (like a hash → list of video locations map). Once we have all matching frame pairs, then look for diagonal lines in the time-index space (which correspond to sequences in order). This is akin to the classic longest common subsequence problem, but with tolerance for slight mismatches (hence more like DTW). Dynamic programming (like Smith-Waterman algorithm) could find the longest aligned subsequence of frames, but that might be overkill unless we restrict to likely pairs.

A more scalable approach: use MinHash on frame sets. Represent each video by the set of its frame hashes (or better, frame hash shingles of length k). Compute a MinHash signature for each video that approximates the Jaccard similarity of frame sets. If any pair of videos has a Jaccard similarity above a low threshold (0.1 in our case for 10% overlap), they are candidates for partial overlap. This way, we quickly filter candidates and then do a detailed check of alignment on those pairs. MinHash + LSH can bucket potentially overlapping videos efficiently. The trade-off is that MinHash loses ordering information, so it might flag videos that share content out-of-order (e.g., both contain some scenes from the same source but in different sequence – which might be fine to consider duplicates anyway).

Additionally, consider using the audio approach for subset: if video B’s audio is wholly contained in video A’s audio (detected by matching a segment of Chromaprint fingerprint), that’s a strong indicator B is a subclip of A. This can complement the visual subset detection.



By implementing such subset detection, vdedup will catch cases like: a full movie vs. a clip from that movie, a YouTube video vs. the same video with an intro added, or overlapping news segments, etc. We also need to avoid false positives: for example, many TV episodes start with the same intro sequence. A 1-minute identical intro across episodes is ~5% of a 20-minute episode, below our threshold, so ideally we would not tag all episodes as duplicates of each other. The solution is to enforce the ≥10% rule strictly and perhaps require that the matched segment is contiguous. In the intro example, yes the intros match but that’s only 5% – vdedup should then ignore it. We ensure this by checking the length of the matched frame sequence vs. each video’s length. Only if match_length >= 0.1 * min(videoA_length, videoB_length) (or perhaps relative to the shorter video) do we consider it a valid duplicate. This will filter out short commonalities.

To implement pHash-based subset search efficiently, we can use data structures: e.g., a Trie of frame hashes for each video, or suffix arrays, but those might be overkill. Simpler: use a rolling hash for sequences. We can assign each pHash a small ID (by clustering similar pHashes to account for slight differences), then each video becomes a sequence of IDs. Use a substring search (like KMP algorithm or Rabin-Karp rolling hash) to find if sequence of length k from video B appears in video A’s sequence. Even k=3 or 4 consecutive frame matches might be enough to confirm an overlap, given the low probability of random match. This approach could be done after initial candidates are found to precisely locate where the overlap is.

In summary, better sampling plus sequence matching will greatly improve subset detection. By covering the timeline thoroughly with hashes and employing algorithms to detect long runs of matching hashes, vdedup can reliably identify when videos share sizable chunks of content. These improvements ensure that even if a video is hidden as part of a larger video, it won't slip through undetected.

Confidence Scoring and Verification Mode

When duplicates or overlaps are found, vdedup should assign a confidence score to each match and facilitate a verification step so users can review and confirm deletions:

Confidence Scoring: Each potential duplicate pair (or group) can be scored on how strong the match is. Several factors can contribute to a composite confidence:

Visual Similarity Score: If using perceptual hashes, this could be based on average Hamming distance of matching frame hashes (lower distance = more confidence). If using deep features, we can use the normalized distance (or inner product) between the video embeddings or the percentage of frames in one video that found close neighbors in the other. For instance, the vector-based method in the Milvus tutorial defines video similarity as the average of maximum frame-to-frame similarities. We could adopt a similar metric: for each frame of Video A, find the most similar frame in Video B (via embedding similarity), average those values, and get a score 0–1. Higher means more of A is visually similar to B. A simpler proxy: “X% of frames of A have a near-duplicate in B” and vice versa.

Overlap Percentage: For partial overlaps, compute the fraction of the video’s duration that overlaps. If Video B is fully inside Video A, that might be 100% of B and, say, 30% of A; we could take the average (65%) or just note 100% containment of B. This percentage can directly serve as a confidence or at least an explanatory metric. We might classify: >90% overlap = exact duplicate, 50–90% = high overlap, 10–50% = partial match, etc.

Audio Match Score: If audio fingerprints are used, the matching algorithm often provides a score (e.g., number of matching fingerprint entries, or a confidence from 0 to 1). Incorporate this: if both audio and video signals say these are duplicates, confidence is extremely high. If only one modality matches strongly, confidence is moderate.

Metadata Consistency: If in addition to content, certain metadata line up (e.g., same resolution and very close durations), that bumps confidence. Conversely, if two videos have the same name or nearly so, that might increase confidence a bit (though not reliable enough alone).

We can combine these into a single score 0–100 or 0–1. For example, one formula could be a weighted sum: confidence = w1*(video_similarity) + w2*(audio_match) + w3*(overlap_ratio). Where video_similarity could be based on hash matches or feature distance converted to [0,1], audio_match is either 1 or 0 if we confirm audio is same, overlap_ratio is (common_duration / min(duration_A, duration_B)). These weights can be tuned experimentally. In practice, even a simpler rule-based classification might suffice (like if overlap >=10% and features match, mark as duplicate). The confidence is mainly to rank results and to potentially auto-select the most obvious cases.


Verification Mode (User Confirmation): Instead of immediately deleting detected duplicates, vdedup should support a verification step where users can review and confirm the matches. In a CLI context, this could be interactive or output to a report file for manual inspection:

Grouping Results: Present duplicates as groups or pairs. For instance, if video A, B, C are all similar, group them together. Show the user a summary of each file (filename, duration, resolution, size, etc.) along with the confidence score and key differences. This can be done in a Markdown or tabular text output for CLI, or even an HTML report that the user can open in a browser to see thumbnails.

Ranking: Order the groups by confidence or by the amount of space that could be saved. The highest confidence (likely exact duplicates) should be listed first – these are usually safe to remove. Lower confidence (small overlaps or ambiguous cases) come later for closer scrutiny.

Previewing: Ideally, allow the user to preview the duplicate content. For example, vdedup could generate a short GIF or a pair of thumbnails from the matching segment to visually confirm the duplication. If not automated, at least provide timestamps of where the overlap occurs (“Video B appears from 00:10 to 01:00 in Video A”) so the user can manually verify by opening those files. In a console environment, perhaps the tool could prompt: "Duplicate found: A and B (90% similar). Show details? (y/n)" and if yes, perhaps print time intervals of overlap or open an external player at that segment.

User Confirmation: Provide options to the user to mark which files to delete or keep. In a simple form, vdedup can output a list of file paths to delete which the user can pipe to a shell script or confirm. A more interactive CLI might list each group and ask e.g. “Keep [1] A.mp4 or [2] B.mp4 as the master? (the other will be deleted)”. The user then inputs a choice.

Auto-Verification: For users who trust the system or for scripting, an auto mode could delete files above a certain confidence threshold (say > 0.95) automatically and flag the rest for manual review. This threshold could be configurable.


Match Ranking Criteria: When multiple duplicates are found, vdedup should also decide which copy is the “best” to keep. We touched on metadata for quality – this can be integrated into verification: by default, mark the lowest quality (resolution/bitrate) item in a duplicate set for deletion. The verification UI can highlight that (“we suggest deleting X (1280x720, 500kbps) and keeping Y (1920x1080, 1200kbps)”). The confidence score helps ensure we only do this when we’re sure X and Y are actually the same content.


By providing a verification mode, we prevent potential false positives from causing data loss. Even with high recall methods, there’s a small chance of a coincidental similarity or a shared segment that doesn’t mean the videos are true duplicates (for example, two different videos both contain a common public domain clip). The user review step catches these. It also gives the user control – maybe they actually want to keep both copies if one is a subset (one might be a highlight reel of a longer video that they want to preserve separately). The system’s job is to present the information clearly and make a recommendation (via confidence and quality metrics), but allow final human confirmation.

From an implementation standpoint, generating a report file in Markdown or JSON might be a good approach for CLI consumption (as requested). The report can list each duplicate group, the members, their scores, and suggested action. This could then be parsed by a script or just read by the user. Ensuring the format is structured (maybe machine-readable JSON plus a human-friendly summary) will help integration with other tools or automation.

Architectural Refinements and New Stages

Bringing all these pieces together, we propose an improved architecture for vdedup with new processing stages and algorithms:

1. Preprocessing & Ingestion Stage:

Probe Metadata: Quickly gather video metadata (resolution, length, codec) using ffprobe. This informs later decisions (e.g., sampling rate, or if two videos have identical length which is a clue).

Frame Sampling & Scene Detection: Decode video using FFmpeg (with hardware accel if available) to retrieve frames. Perform scene cut detection (via content change or using keyframes) to identify unique segments. Sample frames at scene boundaries and at regular intervals within long scenes. This yields a set of representative frames for the video (aim for a diverse coverage of the video’s content).

Audio Extraction: If audio is present, extract the audio track (could use FFmpeg to get raw PCM or a downmixed mono WAV). This will be used in the audio fingerprint stage.



2. Feature Extraction Stage:

Perceptual Hashing: Compute perceptual hashes (pHash or others) for each sampled frame. Store these in an array for the video. Possibly also compute a quick hash for the whole video (like VideoHash’s 64-bit signature) for a first-pass comparison of full-video similarity.

Deep Feature Embedding: For a selection of frames (could be the same ones used for pHash, or a subset if too many), compute deep CNN embeddings using PyTorch (e.g., ResNet). Optionally, aggregate to a video-level embedding (average or max of frame vectors). Normalize these features and prepare them for indexing (might reduce dimensionality with PCA).

Audio Fingerprint: Compute Chromaprint fingerprint for the audio. This can produce either a full fingerprint or a set of fingerprints for segments. For example, run fpcalc to get the fingerprint string of the whole audio; if needed, split the audio into overlapping chunks (like 60-second windows) and fingerprint each to detect local matches.

These features (hashes, embeddings, audio prints) can be cached on disk (e.g., in a JSON or binary file) so that we don’t need to re-extract them on subsequent runs.



3. Indexing Stage:

Hash Index: Build an index/dictionary for pHashes. One method: round each 64-bit pHash to a shorter code (or use LSH on them) and use a hashtable mapping from code to list of (video, frame_time) occurrences. This allows quickly finding candidate matching frames between videos. For near-hash matching (within a Hamming radius), we can use LSH or bit masks to find neighbors. Another approach is to use a BK-tree data structure for hashes which efficiently finds close matches by Hamming distance.

Vector Index: Use FAISS to index deep embeddings. If memory permits, index all frame vectors; otherwise, index video-level vectors for a coarse search. FAISS can be configured for approximate search (trade a tiny bit of accuracy for speed) which is fine for our case since we’ll verify matches anyway. We could have two indices: one for video-level embeddings (to quickly find the top K similar videos for each video), and one for frame embeddings (for detailed partial matching once candidate pairs are identified).

Audio Index: For audio, if using Chromaprint’s full string, we might not need an index – we can directly compare the hashes of each pair (they’re not huge). But if segment fingerprints are used, we could index those too (perhaps in a similar way to frame hashes: map audio fingerprint chunks to the videos they appear in).



4. Candidate Generation Stage:

Using the indices above, generate candidate duplicate pairs/groups:

Full Video Candidates: Compare the global video hashes or embeddings across all videos. For instance, cluster videos by the 64-bit VideoHash – identical or very close hashes are grouped. Also, use video-level embedding distances: for each video, retrieve nearest neighbors (say top 5) from the video vector index. If any neighbor is above a similarity threshold (or below a distance threshold), mark that as a potential match. This stage finds the obvious duplicates and very similar videos quickly, dramatically narrowing the search space.

Frame-Level Candidates for Partials: For each video, take each frame’s perceptual hash and lookup in the hash index to see if the same (or similar) hash appears in any other video. Each hit is evidence of a possible overlap. Tally these: e.g., if Video A has 15 frame-hash matches scattered in Video B, that’s significant. If those matches occur in a continuous sequence (check the timestamps), then A and B share that segment. We could even use a sliding window: take 3 consecutive frame hashes in A, form a tuple, and see if that tuple appears in sequence in B (this reduces random matches). This candidate generation will produce pairs like “Video A and Video B might overlap, roughly around A:10s–40s and B:50s–80s”.

Audio Candidates: Compare audio fingerprints: either via an AcoustID lookup (if online, but we want offline ideally) or by computing a similarity between fingerprint strings. Chromaprint fingerprints can be compared by Hamming distance or by counting common subsequences. If two videos’ audio prints are very similar, output them as candidates. This will catch cases a visual-only approach might miss (e.g., duplicates where video was camcorded but audio was same, etc.).



By the end of this stage, we have a set of candidate pairs or groups that merit detailed comparison. All non-candidates can be assumed distinct enough to ignore (this is our recall filter – we want this stage to be very permissive so as not to miss true duplicates, even if it means some false positives move forward).


5. Detailed Comparison & Scoring Stage:

For each candidate pair (or group), perform a verification analysis. This involves:

If not already aligned, align the two videos along a timeline of matching content. Using the frame matching info, find the longest matching segment(s). We might do a mini sequence alignment here to refine start/end of overlap. For example, if frame hashes matched at points, decode a short segment around those points and compute a finer similarity (like SSIM for each frame or difference in pixels) to be sure and to get exact overlap boundaries.

Compute the metrics for confidence: percentage overlap, average feature similarity, etc., as discussed in the prior section. If the pair is part of a larger group (say A, B, C all similar), we might compare all against each other or choose a reference (like compare all to A).

Assign a confidence score. For example, Video A vs B: 95% of A’s frames matched B (maybe B is the same video), score = 0.99. Or A vs C: only a 15% segment matched (partial), score = 0.6. Use whatever scale but keep it consistent.


Also determine the relationship: is it full duplicate (one is essentially contained in the other or they’re both the same length content), or partial overlap? Perhaps tag the pair as one of: {Exact duplicate, Subset, Overlap, Similar, False positive}. This can be based on the overlap fraction and confidence.



6. Output Stage (Reporting & User Interaction):

Aggregate results by grouping files that are mutual duplicates. Then either present to user (interactive CLI or generate a report).

If interactive, implement prompts as described: list group, ask which to keep. If automatic, apply the keep rules (e.g., keep highest resolution) and list the ones marked for deletion.

The output could be a structured JSON with all match info (for programmatic use) and a nicely formatted Markdown/text summary for the user. For each group, include the confidence and key metadata. Possibly also print commands or scripts to delete the files if the user agrees (making it as easy as copy-paste to execute the deletion).




Each of these stages corresponds to a logical separation of concerns: feature extraction, indexing, candidate search, verification, and user confirmation. This modular design makes it easier to maintain and extend (for example, adding a new hashing algorithm or a new type of feature is just adding to stage 2 and perhaps an index in stage 3).

We also incorporate the new techniques mentioned: sequence-based hashing (stage 4 with frame sequence tuples), MinHash/LSH (stage 3 for set similarity indexing), and vector embeddings (stage 2 & 3 with deep features and FAISS). These additions directly address the weaknesses of a basic pHash-only approach by adding redundancy and more robust matching methods.

In terms of open-source best practices, this architecture uses proven components (FFmpeg, OpenCV, PyTorch, FAISS, Chromaprint) at their strengths and avoids duplicating what’s available. It’s a pipeline that can be run in a CLI environment, and each step’s results (like caches or intermediate indices) can be saved for debugging or incremental runs.

Code-level notes: We would implement this in Python (given vdedup appears to be Python-based) with perhaps C++ extensions for performance-critical parts if needed (FAISS is in C++ under the hood, and we might write a small C extension for very tight inner loops if profiling shows a bottleneck). Use multiprocessing to handle multiple videos in parallel during feature extraction. The FAISS index can be persisted to disk (faiss has I/O for indexes) so that the expensive indexing doesn’t have to redo every run. Also, be mindful of memory – perhaps load and process one video at a time in feature extraction to avoid holding all frames of all videos in RAM.

By following this refined architecture, the vdedup system will be far more powerful and accurate: it will detect near-duplicates with high recall (even if only 10% overlaps), scale to large collections by using efficient search, allow the user to configure speed/accuracy, and integrate seamlessly with open-source tools for a robust implementation. All recommendations here are grounded in techniques used by state-of-the-art systems and research (as evidenced by similar tools and literature) and should greatly improve vdedup’s deduplication capabilities.
