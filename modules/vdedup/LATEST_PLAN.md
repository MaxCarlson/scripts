# **Architectural Modernization of Video Deduplication Systems: A Comprehensive Technical Report and Implementation Strategy for Autonomous Agents**

## **1\. Introduction: The Exabyte Scale Challenge**

The digital ecosystem is currently witnessing an unprecedented proliferation of video content, driven not only by the democratization of capture devices but, more significantly, by the advent of generative artificial intelligence. As of late 2025, the volume of video data is expanding at a rate that outpaces the growth of traditional storage infrastructure, necessitating a fundamental reimagining of data management strategies. With Foundation Models and Large Language Models (LLMs) requiring trillion-token datasets for training 1, the purity and uniqueness of data have become paramount. Duplicate data does not merely consume storage; it actively degrades the performance of training pipelines, introduces bias, and increases the computational cost of downstream inference tasks.  
The traditional paradigms of deduplicationâ€”relying on exact byte-matching cryptographic hashes like MD5 or SHA-256â€”have been rendered effectively obsolete in the context of modern video workflows. In an era where content is routinely transcoded, resized for different social platforms, subjected to minor color grading, or watermarked, two files can be bitwise distinct yet perceptually and semantically identical. This phenomenon, known as the "near-duplicate" problem, requires systems that can "see" the video content rather than just reading its binary footprint.2  
Furthermore, the operational context of software development itself is shifting. The rise of "Agentic AI"â€”autonomous software agents capable of executing complex, multi-step engineering tasks via Command Line Interfaces (CLIs)â€”demands that we design tools not just for human operators, but for machine intermediaries.4 A modern video deduplication system must therefore be architected with dual interfaces: a high-performance, hardware-accelerated core for data throughput, and a semantic, structured surface for agentic interaction.  
This report provides an exhaustive analysis of the architectural requirements for a state-of-the-art video deduplication system as of 2025\. It synthesizes research on high-performance computing, deep learning-based computer vision, and embedded vector database technologies. The ultimate objective is to formulate a granular, executable blueprint that a CLI-based Large Language Model can follow to upgrade legacy repositories into modern, industrial-grade solutions.

### **1.1 The Evolution from Perceptual to Semantic Analysis**

Historically, "fuzzy" deduplication relied on perceptual hashing (pHash) algorithms. These methods utilize discrete cosine transforms (DCT) or wavelet transforms to generate a "fingerprint" of an image that remains stable despite compression artifacts or minor resizing.6 While computationally efficient, standard perceptual hashing struggles with the complex transformations common in 2025, such as aspect ratio changes (e.g., converting landscape video to vertical formats for mobile consumption), substantial cropping, or the insertion of picture-in-picture overlays.  
Recent benchmarks and academic literature highlight a migration toward "Semantic Deduplication." This approach leverages deep learning embeddingsâ€”high-dimensional vectors derived from models like CLIP (Contrastive Language-Image Pre-training) or SigLIPâ€”to capture the *concept* of the video frame rather than its visual texture.3 For instance, a semantic deduplicator can identify that a black-and-white clip of a dog jumping is the same content as its color original, a task where traditional pHash would likely fail due to the massive difference in frequency domain representation.  
However, semantic analysis comes with a profound computational penalty. Inference using Vision Transformers (ViT) is orders of magnitude more expensive than calculating a Hamming distance on a binary hash. Therefore, the prevailing architecture in 2025 is a "Layered Coarse-to-Fine" approach: utilizing ultra-fast hashing or metadata filtering to eliminate 99% of non-matches, followed by heavy neural inference only on the remaining candidates.2

### **1.2 The Computational Bottleneck: Decode and Transport**

Implementing layered deduplication at scale exposes a critical infrastructure bottleneck: video decoding. Uncompressing video is a computationally intensive process that, in naive Python implementations (often relying on ffmpeg-python wrappers), occurs on the CPU.10 This architecture forces raw video framesâ€”which are significantly larger than their compressed counterpartsâ€”to be copied from system RAM to GPU memory for inference. This data movement saturates the PCIe bandwidth and leaves the powerful GPUs idling while they wait for the CPU to feed them data.11  
To achieve the throughput required for terabyte- or petabyte-scale datasets, the system must adopt a "Zero-Copy" pipeline. This involves performing the decoding directly on the GPU's dedicated hardware engines (NVDEC) and passing the memory pointers directly to the inference engine (e.g., PyTorch) without the data ever touching the host CPU or system RAM.13

### **1.3 Scope of the Report**

This document is structured to guide the transformation of a theoretical or legacy codebase into a high-performance system. The following sections will detail:

* **Theoretical Frameworks:** Deep dives into hashing algorithms and vector embeddings.  
* **Hardware Architecture:** Leveraging NVIDIA libraries for hardware acceleration.  
* **Data Management:** Implementing sqlite-vec for embedded, serverless vector search.  
* **Sampling Strategy:** Moving beyond uniform sampling to entropy and motion-based keyframe extraction.  
* **Implementation Blueprint:** A rigorous, step-by-step instruction set designed for an AI agent to execute.

## **Execution Checklist (Bottom-Up Tracking)**

Working from the bottom of this document upward ensures we do not miss downstream dependencies before executing earlier-stage work. Progress is tracked with Markdown checkboxes so newly completed milestones can be recorded quickly.

- [x] **Â§9 References curated** â€“ Bibliography validated and cross-linked to the research artifacts that informed this plan.  
- [x] **Â§8 Conclusion captured** â€“ Future outlook authored and aligned with the modernization scope.  
- [x] **Â§7.6 Benchmark dataset harness** â€“ Reproducible YouTube-seeded dataset tooling (`tests/generate_media_dataset.py`), evaluator (`tests/evaluate_media_dataset.py`), manifests, and opt-in pytest wiring are in place.  
- [ ] **Â§7.5 Agentic CLI wrapper** â€“ Typer-based CLI with ingest/report/delete workflows still needs to be implemented per blueprint.  
- [ ] **Â§7.4 Vector database layer** â€“ sqlite-vec backed storage with hybrid hash/vector queries is not yet integrated.  
- [ ] **Â§7.3 Semantic & hashing engine** â€“ CLIP-driven embeddings plus perceptual hashing pipeline remains pending.  
- [ ] **Â§7.2 Hardware-accelerated decoder** â€“ NVDEC/PyNvVideoCodec based zero-copy decode path has not been wired up.  
- [ ] **Â§7.1 Environment & dependencies** â€“ Production-ready dependency graph (PyNvVideoCodec wheels, torch, sqlite-vec, etc.) still needs to be formalized.  
- [x] **Â§6 Agentic design principles** â€“ CLI/UX guidance for LLM agents documented and ready for implementation.  
- [x] **Â§5 Vector storage architecture** â€“ Target schema, sqlite-vec integration strategy, and candidate query patterns defined.  
- [x] **Â§4 Sampling methodologies** â€“ Entropy, shot-boundary, and adaptive stride concepts formalized for later execution.  
- [x] **Â§3 Hardware pipeline research** â€“ Zero-copy GPU design described, including NVDEC, DLPack, and concurrency considerations.  
- [x] **Â§2 Theoretical framework** â€“ Hybrid similarity model (cryptographic, perceptual, semantic) grounded in literature.  
- [x] **Â§1 Problem framing** â€“ Motivation, scope, and constraints articulated for exabyte-scale deduplication.

## **2\. Theoretical Framework: Algorithms for Similarity Detection**

The core of any deduplication system is the algorithm used to define "similarity." In 2025, relying on a single metric is insufficient. Robust systems employ a composite signature comprising cryptographic, perceptual, and semantic elements.

### **2.1 Perceptual Hashing Mechanics**

Perceptual hashing remains the first line of defense due to its speed. Unlike cryptographic hashes, where a single bit change results in a completely different output (the avalanche effect), perceptual hashes are locality-sensitive: similar inputs produce similar hashes.

#### **2.1.1 Algorithm Selection: dHash vs. wHash**

Research indicates that different hashing algorithms offer varying resilience to specific attacks or transformations:

* **Average Hash (aHash):** The simplest form, which reduces the image to a low-resolution grayscale grid (e.g., 8x8) and compares each pixel to the mean. It is fast but extremely fragile to gamma correction or histogram equalization.15  
* **Difference Hash (dHash):** improved upon aHash by tracking the *gradient* (change in brightness) between adjacent pixels rather than absolute values. This makes dHash robust to brightness and contrast changes, which are common in video re-encoding.16  
* **Wavelet Hash (wHash):** Utilizes the Discrete Wavelet Transform (DWT). It operates in the frequency domain, separating high-frequency noise from low-frequency structure. wHash is generally considered the state-of-the-art for traditional perceptual hashing as it is resilient to blurring and compression artifacts (like JPEG blocking).7

The videohash library, a Python reference implementation, utilizes a specific tiling strategy. It extracts one frame per second, shrinks them to 144x144, tiles them into a collage, and then computes the hash of the collage.7 While innovative, this approach introduces fragility: if a video is trimmed by a few seconds, the collage shifts, and the hash changes entirely. A modern system must hash individual keyframes to allow for subsequence matching.17

#### **2.1.2 The Role of Watermarking**

Digital watermarking is often discussed in the context of deduplication and provenance. However, benchmarks from 2025 suggest that watermarking is ineffective for robust deduplication in "wild" datasets. Watermarks are typically embedded in high-frequency components that are destroyed by aggressive compression algorithms used by social media platforms.18 Therefore, deduplication must rely on the content itself (fingerprinting) rather than injected metadata.

### **2.2 Semantic Embeddings and Vector Space**

When perceptual hashes failâ€”such as when a video is cropped to 9:16 aspect ratio from a 16:9 originalâ€”semantic embeddings provide a fallback. These vectors represent the image in a high-dimensional abstract space.

#### **2.2.1 CLIP and its Successors**

The Contrastive Language-Image Pre-training (CLIP) architecture projects images and text into a shared embedding space. The "closeness" of two vectors is measured by Cosine Similarity.

* **Resilience:** CLIP embeddings are remarkably robust to stylistic changes. A cartoon of a cat and a photo of a cat may be close in vector space, allowing for "conceptual deduplication".3  
* **Optimization:** Standard CLIP models (ViT-L/14) are heavy. For deduplication, optimized variants or distilled models (like MobileCLIP or smaller SigLIP variants) are preferred to increase throughput.19  
* **Batching Implications:** Processing video frames requires accumulating tensors into batches to saturate the GPU. Processing frames one-by-one is inefficient due to kernel launch overheads.20

### **2.3 Hybrid Similarity Metrics**

The most effective deduplication logic in 2025 combines these signals using a tiered decision tree:

| Metric | Cost | False Positive Rate | False Negative Rate | Use Case |
| :---- | :---- | :---- | :---- | :---- |
| **MD5/SHA256** | Negligible | 0% | High (fails on 1 bit change) | Exact file duplicates. |
| **wHash (Hamming)** | Low | Low | Medium (fails on crop/rotation) | Re-encoded videos, slight compression. |
| **CLIP (Cosine)** | High | Medium (semantic drift) | Low | Crops, overlays, heavy edits, B\&W conversion. |

**Strategic Insight:** The "Layered Coarse Resolution" approach 2 suggests using wHash to filter the search space. If the Hamming distance is $\> X$ but $\< Y$ (the "ambiguous zone"), the system should trigger the CLIP inference. If Hamming distance is very low, CLIP can be skipped, saving compute.

## **3\. Hardware Acceleration and Pipeline Architecture**

The primary barrier to implementing the theoretical framework above is computational throughput. Video data is voluminous; decoding 4K video at 60fps requires processing gigabytes of raw pixel data per second.

### **3.1 The Python-FFmpeg Bottleneck**

The standard Python video stack (opencv-python, moviepy, ffmpeg-python) typically wraps the C-based FFmpeg library. While FFmpeg itself is fast, the binding usually involves:

1. FFmpeg decoding the frame to CPU RAM.  
2. Python converting the memory buffer to a NumPy array.  
3. PyTorch/TensorFlow converting the NumPy array to a Tensor.  
4. Moving the Tensor from CPU to GPU.

This "buffer shuffle" dominates the processing time. Benchmarks indicate that for high-resolution video, the PCIe bus bandwidth becomes the bottleneck, limiting GPU utilization to as low as 20-30%.10

### **3.2 NVIDIA PyNvVideoCodec and NVDEC**

To address this, NVIDIA introduced PyNvVideoCodec (an evolution of the Video Processing Framework). This library exposes the NVDEC (NVIDIA Decoder) hardware directly to Python.11

#### **3.2.1 Mechanics of Hardware Decoding**

NVDEC is a dedicated ASIC on the GPU die, separate from the CUDA cores. Using it allows the CUDA cores to remain free for the embedding inference tasks.

* **Throughput:** NVDEC can decode multiple streams in parallel, achieving thousands of frames per second in aggregate.10  
* **Memory Management:** The critical feature of PyNvVideoCodec is the use\_device\_memory=True flag. When set, the decoded frame is placed directly into VRAM (Video RAM).

### **3.3 Zero-Copy Tensor Interoperability (DLPack)**

Once the frame is in VRAM, it must be passed to the Deep Learning framework (PyTorch) without copying. This is achieved via **DLPack**, an open standard for tensor memory sharing.10  
The workflow is as follows:

1. **Decode:** PyNvDecoder produces a Surface in VRAM (typically in NV12 YUV format).  
2. **Format Conversion:** The inference model expects RGB Planar data. Instead of using CPU-based OpenCV for this conversion, PyNvVideoCodec provides PySurfaceConverter. This uses the GPU to convert NV12 to RGB Planar instantly.14  
3. **Encapsulation:** The RGB Surface offers a \_\_dlpack\_\_ method.  
4. **Ingestion:** torch.from\_dlpack(surface) creates a PyTorch tensor that *points* to the existing memory address of the surface.  
5. **Inference:** The tensor is normalized and fed to the model.

**Implication:** The raw video data *never* leaves the GPU memory. Only the final embedding vector (e.g., 512 floats, \~2KB) is transferred back to the CPU for storage. This architecture can improve total throughput by 5x-10x compared to CPU-based decoding pipelines.10

### **3.4 Handling Concurrency and the GIL**

While PyNvVideoCodec releases the Global Interpreter Lock (GIL) during C++ execution 22, the surrounding Python code (database logic, file I/O) is still single-threaded. To maximize NVDEC saturation, the architecture should employ a ThreadPoolExecutor.

* **Thread 1:** Manages file discovery and database queries (I/O bound).  
* **Thread 2:** Manages the GPU pipeline (Decoder \-\> Converter \-\> Embedder).  
* **Queues:** Use queue.Queue to buffer file paths between the discovery thread and the processing thread.

## **4\. Advanced Frame Sampling Methodologies**

Analyzing every frame of a 60fps video is redundant; consecutive frames are often nearly identical. However, the naive approach of "Uniform Sampling" (e.g., 1 frame every second) is flawed. It may capture a blurry transition frame or miss a short but distinct scene entirely.23

### **4.1 Content-Aware Sampling Strategies**

#### **4.1.1 Entropy-Based Filtering**

Information theory tells us that frames with higher entropy contain more "information" (detail/texture) than low-entropy frames (e.g., a black screen or a flat blue sky).

* **Algorithm:** During the decoding process, compute a lightweight histogram or entropy score.  
* **Logic:** If $Entropy(Frame\_t) \< Threshold$, skip the frame. This prevents the database from being flooded with embeddings of black screens (fades) which would match almost every other video's fade-out.25

#### **4.1.2 Scene Boundary Detection (Shot Detection)**

The "Centroid" approach suggests that the most representative frame of a scene is the one in the middle, or the one most similar to all other frames in that scene.2

* **Implementation:** While full scene detection is expensive, PyNvVideoCodec and NVDEC expose motion vectors. A sudden spike in motion vector magnitude or a complete change in intra-coded macroblocks usually signals a scene change (or cut).  
* **Strategy:** The sampler should prioritize "I-Frames" (Keyframes) identified by the codec, as these are the anchors of the video stream and usually represent high-quality imagery.17

### **4.2 Dynamic Striding**

The system should implement **Adaptive Frame Sampling (AFS)**.24

* Start with a stride of 1 second.  
* Compare $Frame\_t$ and $Frame\_{t-1}$ using a cheap metric (e.g., pixel difference on downscaled GPU surface).  
* If difference \> High\_Threshold (fast action), decrease stride to 0.5s to capture detail.  
* If difference \< Low\_Threshold (static scene/interview), increase stride to 5s to save compute.

## **5\. Vector Storage and Retrieval Architecture**

Storing millions of high-dimensional vectors requires a specialized database. Traditional RDBMS (PostgreSQL, MySQL) are inefficient for Nearest Neighbor search. While specialized Vector DBs (Milvus, Qdrant) exist, they introduce external dependencies and "server" management overhead that complicates CLI tools.

### **5.1 The Rise of Embedded Vector Databases: sqlite-vec**

In 2024-2025, the ecosystem shifted towards *embedded* vector search. sqlite-vec is an extension for SQLite that allows vector operations directly within the local .db file.27

* **Advantages:** Zero dependency (bundled with the application), ACID compliant, single-file portability.  
* **Performance:** It uses SIMD instructions (AVX/NEON) for distance calculations, making it competitive with FAISS for datasets up to a few million vectors.27

### **5.2 Schema Design for Hybrid Deduplication**

The database schema must support the hybrid approach (Hash \+ Vector).  
**Table: videos**

| Column | Type | Description |
| :---- | :---- | :---- |
| id | INTEGER PK | Unique ID. |
| path | TEXT | Absolute file path. |
| file\_hash | TEXT | MD5 of the first 64KB (Quick check). |
| duration | REAL | Video duration in seconds. |
| metadata | JSON | Codec, resolution, fps. |

**Table: frames (Standard Data)**

| Column | Type | Description |
| :---- | :---- | :---- |
| id | INTEGER PK |  |
| video\_id | INTEGER FK | Link to parent video. |
| timestamp | REAL | Time of the frame. |
| phash | BLOB | 64-bit perceptual hash (wHash). |

**Virtual Table: vec\_frames (Vector Data using vec0)**

| Column | Type | Description |
| :---- | :---- | :---- |
| rowid | INTEGER FK | Links to frames.id. |
| embedding | FLOAT | The CLIP embedding vector. |

### **5.3 Query Optimization**

To find duplicates for a new frame $F\_{new}$:

1. **Hamming Filter:** SELECT rowid FROM frames WHERE hamming\_distance(phash,?) \< 5\. (This requires a custom SQLite function or bitwise math: BIT\_COUNT(phash ^?)).  
2. **Vector Search:** If no hash match, SELECT rowid FROM vec\_frames WHERE embedding MATCH? AND k=5 ORDER BY distance.  
3. **Temporal Consistency:** A single frame match is noise. A true video duplicate requires a *sequence* of matches. The application logic must verify that if Frame $T$ matches Frame $T'$, then Frame $T+1$ matches Frame $T'+1$.17

## **6\. Designing for Agentic Implementation**

Creating tools for AI Agents (LLMs) requires a shift in design philosophy. Agents do not "see" terminal output the way humans do; they parse text streams.

### **6.1 Principles of CLI Design for Agents**

1. **Machine-Readable Output:** The tool must support a \--json or \--format machine flag. This suppresses progress bars (which pollute the context window with carriage returns) and outputs pure, structured JSON.5  
2. **Idempotency and State:** Agents often retry commands. Operations like "scan directory" must be idempotentâ€”skipping files already in the DB rather than crashing or re-processing them.  
3. **Explicit Error Codes:** Semantic exit codes help the agent decide what to do next. (e.g., Exit Code 0 \= Success, 1 \= Generic Error, 2 \= Database Locked, 3 \= GPU OOM).  
4. **Self-Description:** The tool should implement a \--schema command that dumps the JSON schema of its output. This allows the agent to understand exactly what data structure to expect without guessing.4

### **6.2 Context Management**

LLMs have finite context windows. The tool's output must be concise.

* **Bad Output:** Printing the full embedding vector to stdout (thousands of floats).  
* **Good Output:** Printing the video\_id and a summary (e.g., "5 duplicates found").

## **7\. Implementation Blueprint: The "AutoDedup" System**

This section serves as the direct instruction set for the CLI LLM agent. It is organized into logical implementation phases.

### **7.1 Phase 1: Environment and Core Dependencies**

**Objective:** Establish a robust Python environment with hardware acceleration support.  
**Agent Instructions:**

1. **Project Structure:** Initialize a directory structure:  
   autodedup/  
   â”œâ”€â”€ core/ (decoding, hashing, embedding)  
   â”œâ”€â”€ db/ (sqlite-vec storage)  
   â”œâ”€â”€ cli/ (interface)  
   â””â”€â”€ utils/ (logging, config)

2. **Dependency Configuration:** Create a pyproject.toml.  
   * **Crucial:** You cannot simply pip install PyNvVideoCodec. You must specify the NVIDIA PyPI index.  
   * Add torch, torchvision, transformers, pillow, typer, rich, sqlite-vec.  
   * For perceptual hashing, include imagehash (which implements wHash/dHash).  
3. **Conditional Imports:** In core/\_\_init\_\_.py, implement a check for torch.cuda.is\_available(). If false, set a global flag USE\_CPU \= True. The system must not crash on non-NVIDIA machines; it should fallback to cv2 (OpenCV) decoding, albeit slower.

### **7.2 Phase 2: The Hardware-Accelerated Decoder Module**

**Objective:** Implement the Zero-Copy pipeline.  
**Agent Instructions:**

1. **File:** Create core/decoder.py.  
2. **Class:** VideoDecoder.  
3. **Initialization:**  
   * Accept file\_path, gpu\_id.  
   * Initialize nvc.PyNvDecoder with use\_device\_memory=True. This is non-negotiable for performance.13  
4. **Surface Conversion:**  
   * Initialize nvc.PySurfaceConverter.  
   * Configure it to convert NV12 (Decoder output) to RGB (Model input) directly on GPU.  
   * *Insight:* CLIP models often expect RGB, while video is YUV. Doing this conversion on CPU is a major bottleneck.  
5. **DLPack Export:**  
   * Implement a method next\_batch(batch\_size) that yields a torch.Tensor.  
   * Use surface.ToDLPack() and torch.from\_dlpack().  
   * **Memory Safety:** Ensure the tensor is cloned or processed before the underlying surface is released by the decoder, or use the decoder's built-in buffer management to hold the reference.

### **7.3 Phase 3: The Semantic & Hashing Engine**

**Objective:** Generate hybrid signatures.  
**Agent Instructions:**

1. **File:** Create core/engine.py.  
2. **Models:** Load CLIPModel (e.g., openai/clip-vit-base-patch32) from transformers. Move to GPU.  
3. **Preprocessing:**  
   * Define a torch.nn.Sequential block for resizing (224x224) and normalization.  
   * Apply this to the batch of tensors coming from VideoDecoder.  
4. **Hashing (CPU Side):**  
   * Since wHash requires frequency domain transforms often not implemented in pure PyTorch, this step may require moving a *small* downscaled copy of the frame to CPU.  
   * *Optimization:* Resize the frame to 64x64 on GPU, *then* move to CPU for imagehash.whash(). This minimizes PCIe transfer.  
5. **Batch Processing:**  
   * The process\_video function should accumulate frames until BATCH\_SIZE (e.g., 32\) is reached, run CLIP inference, and compute hashes in parallel.

### **7.4 Phase 4: The Vector Database Layer**

**Objective:** Persistent, searchable storage.  
**Agent Instructions:**

1. **File:** Create db/storage.py.  
2. **Connection:** sqlite3.connect("library.db").  
3. **Load Extension:** db.enable\_load\_extension(True) \-\> sqlite\_vec.load(db).  
4. **Schema Creation:** Implement the schema defined in Section 5.2.  
5. **Insertion Logic:**  
   * Use executemany for batch inserting frames to minimize transaction overhead.  
   * Serialize embedding (numpy array) to bytes/blob for storage if sqlite-vec requires it, or use the vec\_f32 SQL function helper.  
6. **Search Logic:**  
   * Implement find\_candidates(embedding\_vector):  
     SQL  
     SELECT video\_id, distance  
     FROM vec\_frames  
     WHERE embedding MATCH?  
     AND k \= 20  
     ORDER BY distance

   * Use a threshold (e.g., distance \< 0.15) to determine a "hit."

### **7.5 Phase 5: The Agentic CLI Wrapper**

**Objective:** The user interface.  
**Agent Instructions:**

1. **File:** Create cli/main.py using typer.  
2. **Command: ingest:**  
   * Args: directory, \--recursive, \--json-output.  
   * Logic: Walk directory, skip processed files (check DB), run pipeline.  
   * Output: If \--json-output, print {"status": "completed", "processed": 50, "errors":}.  
3. **Command: dedup:**  
   * Args: \--threshold, \--action \[report|delete|symlink\].  
   * Logic: Identify clusters of duplicates.  
   * *Smart Pruning:* If duplicates are found, check resolution/bitrate. Suggest keeping the highest quality version.  
4. **Error Handling:**  
   * Wrap the GPU decode loop in try...except RuntimeError. NVDEC can be flaky with corrupted video files. Catch the error, log it to errors list, and continue. Do not crash the agent's long-running process.

## **8\. Conclusion and Future Outlook**

The modernization of video deduplication moves the discipline from simple file management to complex computer vision engineering. By adopting the architecture proposed in this reportâ€”specifically the use of NVIDIA PyNvVideoCodec for zero-copy decoding and sqlite-vec for embedded semantic searchâ€”developers can build systems capable of handling the exabyte-scale workloads of the generative AI era.  
For the Agentic implementation, the key lies in the strict separation of the high-performance core (C++/CUDA wrapped in Python) from the control logic (CLI). This ensures that while the agent orchestrates the workflow, the heavy lifting is offloaded to dedicated hardware accelerators, achieving the optimal balance of intelligence and raw throughput. This blueprint provides a complete path to updating legacy repositories to this state-of-the-art standard.

## ---

**9\. Appendix: Summary of Key Technologies**

| Technology | Role | Advantage | Key Source |
| :---- | :---- | :---- | :---- |
| **PyNvVideoCodec** | Video Decoding | Zero-copy GPU access, bypasses CPU. | 11 |
| **DLPack** | Data Transport | Cost-free memory sharing between decoder and PyTorch. | 10 |
| **CLIP / SigLIP** | Semantic Analysis | Detects conceptual duplicates (B\&W, crops). | 3 |
| **wHash (Wavelet)** | Perceptual Filtering | Fast, robust to compression, frequency-based. | 7 |
| **sqlite-vec** | Vector Storage | Embedded, serverless, SIMD-accelerated search. | 27 |
| **Entropy Sampling** | Frame Selection | Filters low-information frames (black screens). | 23 |

#### **Works cited**

1. Data Deduplication at Trillion Scale: How to Solve the Biggest Bottleneck of LLM Training, accessed December 31, 2025, [https://zilliz.com/blog/data-deduplication-at-trillion-scale-solve-the-biggest-bottleneck-of-llm-training](https://zilliz.com/blog/data-deduplication-at-trillion-scale-solve-the-biggest-bottleneck-of-llm-training)  
2. Video Deduplication Using Clustering and Hashing-Based Layered Coarse Resolution Approach for Cloud Storage | Semantic Scholar, accessed December 31, 2025, [https://www.semanticscholar.org/paper/Video-Deduplication-Using-Clustering-and-Layered-Chaudhari-Aparna/52794b9633cacef55c9537e302febb5216e10ff3](https://www.semanticscholar.org/paper/Video-Deduplication-Using-Clustering-and-Layered-Chaudhari-Aparna/52794b9633cacef55c9537e302febb5216e10ff3)  
3. Semantic-Aware Image Deduplication: Leveraging Object Recognition for Enhanced Accuracy | Sciety Labs (Experimental), accessed December 31, 2025, [https://labs.sciety.org/articles/by?article\_doi=10.21203/rs.3.rs-6396148/v1](https://labs.sciety.org/articles/by?article_doi=10.21203/rs.3.rs-6396148/v1)  
4. Building a CLI Agent \- Lakshya Agarwal, accessed December 31, 2025, [https://lakshyaag.com/blogs/building-a-cli-agent](https://lakshyaag.com/blogs/building-a-cli-agent)  
5. Keep the Terminal Relevant: Patterns for AI Agent Driven CLIs \- InfoQ, accessed December 31, 2025, [https://www.infoq.com/articles/ai-agent-cli/](https://www.infoq.com/articles/ai-agent-cli/)  
6. pHash.org: Home of pHash, the open source perceptual hash library, accessed December 31, 2025, [https://www.phash.org/](https://www.phash.org/)  
7. akamhy/videohash: Near Duplicate Video Detection (Perceptual Video Hashing) \- Get a 64-bit comparable hash-value for any video. \- GitHub, accessed December 31, 2025, [https://github.com/akamhy/videohash](https://github.com/akamhy/videohash)  
8. Semantic Image Search with OpenAI CLIP and Meta FAISS \- Ultralytics YOLO Docs, accessed December 31, 2025, [https://docs.ultralytics.com/guides/similarity-search/](https://docs.ultralytics.com/guides/similarity-search/)  
9. Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model \- arXiv, accessed December 31, 2025, [https://arxiv.org/html/2502.10248v1](https://arxiv.org/html/2502.10248v1)  
10. PyNvVideoCodec API Programming Guide \- NVIDIA Docs, accessed December 31, 2025, [https://docs.nvidia.com/video-technologies/pynvvideocodec/pynvc-api-prog-guide/index.html](https://docs.nvidia.com/video-technologies/pynvvideocodec/pynvc-api-prog-guide/index.html)  
11. What's New in PyNvVideoCodec 2.0 for Python GPU-Accelerated Video Processing, accessed December 31, 2025, [https://developer.nvidia.com/blog/whats-new-in-pynvvideocodec-2-0-for-python-gpu-accelerated-video-processing/](https://developer.nvidia.com/blog/whats-new-in-pynvvideocodec-2-0-for-python-gpu-accelerated-video-processing/)  
12. Using FFmpeg with NVIDIA GPU Hardware Acceleration, accessed December 31, 2025, [https://docs.nvidia.com/video-technologies/video-codec-sdk/12.0/ffmpeg-with-nvidia-gpu/index.html](https://docs.nvidia.com/video-technologies/video-codec-sdk/12.0/ffmpeg-with-nvidia-gpu/index.html)  
13. PyNvVideoCodec API \- NVIDIA Documentation, accessed December 31, 2025, [https://docs.nvidia.com/video-technologies/pynvvideocodec/pdf/PyNvVideoCodec\_API\_ProgGuide.pdf](https://docs.nvidia.com/video-technologies/pynvvideocodec/pdf/PyNvVideoCodec_API_ProgGuide.pdf)  
14. Exporting video frame to Pytorch tensor Â· NVIDIA/VideoProcessingFramework Wiki \- GitHub, accessed December 31, 2025, [https://github.com/NVIDIA/VideoProcessingFramework/wiki/Exporting-video-frame-to-Pytorch-tensor](https://github.com/NVIDIA/VideoProcessingFramework/wiki/Exporting-video-frame-to-Pytorch-tensor)  
15. JohannesBuchner/imagehash: A Python Perceptual Image Hashing Module \- GitHub, accessed December 31, 2025, [https://github.com/JohannesBuchner/imagehash](https://github.com/JohannesBuchner/imagehash)  
16. GitHub \- benhoyt/dhash: Python library to calculate the difference hash (perceptual hash) for a given image, useful for detecting duplicates \- Reddit, accessed December 31, 2025, [https://www.reddit.com/r/Python/comments/10gbzy5/github\_benhoytdhash\_python\_library\_to\_calculate/](https://www.reddit.com/r/Python/comments/10gbzy5/github_benhoytdhash_python_library_to_calculate/)  
17. Videohash â€“ Perceptual video hashing python package | Hacker News, accessed December 31, 2025, [https://news.ycombinator.com/item?id=28829777](https://news.ycombinator.com/item?id=28829777)  
18. Provenance Detection for AI-Generated Images: Combining Perceptual Hashing, Homomorphic Encryption, and AI Detection Models \- arXiv, accessed December 31, 2025, [https://arxiv.org/html/2503.11195v1](https://arxiv.org/html/2503.11195v1)  
19. Optimizing CLIP Models for Image Retrieval with Maintained Joint-Embedding Alignment, accessed December 31, 2025, [https://arxiv.org/html/2409.01936v1](https://arxiv.org/html/2409.01936v1)  
20. Encoding video frames using CLIP \- ðŸ¤—Transformers \- Hugging Face Forums, accessed December 31, 2025, [https://discuss.huggingface.co/t/encoding-video-frames-using-clip/19054](https://discuss.huggingface.co/t/encoding-video-frames-using-clip/19054)  
21. PyNvVideoCodec \- Get Started \- NVIDIA Developer, accessed December 31, 2025, [https://developer.nvidia.com/pynvvideocodec](https://developer.nvidia.com/pynvvideocodec)  
22. Read Me \- NVIDIA Docs, accessed December 31, 2025, [https://docs.nvidia.com/video-technologies/pynvvideocodec/read-me/index.html](https://docs.nvidia.com/video-technologies/pynvvideocodec/read-me/index.html)  
23. Building High-Quality Video Datasets for GenAI: Scoring, Filtering & Deduplication at Scale, accessed December 31, 2025, [https://medium.com/@asimsultan2/building-high-quality-video-datasets-for-genai-scoring-filtering-deduplication-at-scale-57c22d0fc28f](https://medium.com/@asimsultan2/building-high-quality-video-datasets-for-genai-scoring-filtering-deduplication-at-scale-57c22d0fc28f)  
24. Exploring Video Frame Redundancies for Efficient Data Sampling and Annotation in Instance Segmentation \- CVF Open Access, accessed December 31, 2025, [https://openaccess.thecvf.com/content/CVPR2023W/VDU/papers/Yoon\_Exploring\_Video\_Frame\_Redundancies\_for\_Efficient\_Data\_Sampling\_and\_Annotation\_CVPRW\_2023\_paper.pdf](https://openaccess.thecvf.com/content/CVPR2023W/VDU/papers/Yoon_Exploring_Video_Frame_Redundancies_for_Efficient_Data_Sampling_and_Annotation_CVPRW_2023_paper.pdf)  
25. LemurPwned/video-sampler: Effective frame sampling for ML applications. \- GitHub, accessed December 31, 2025, [https://github.com/LemurPwned/video-sampler](https://github.com/LemurPwned/video-sampler)  
26. ViDeDup: An Application-Aware Framework for Video De-duplication \- USENIX, accessed December 31, 2025, [https://www.usenix.org/event/hotstorage11/tech/final\_files/Katiyar.pdf](https://www.usenix.org/event/hotstorage11/tech/final_files/Katiyar.pdf)  
27. How sqlite-vec Works for Storing and Querying Vector Embeddings | by Stephen Collins, accessed December 31, 2025, [https://medium.com/@stephenc211/how-sqlite-vec-works-for-storing-and-querying-vector-embeddings-165adeeeceea](https://medium.com/@stephenc211/how-sqlite-vec-works-for-storing-and-querying-vector-embeddings-165adeeeceea)  
28. Introducing sqlite-vec v0.1.0: a vector search SQLite extension that runs everywhere \- Reddit, accessed December 31, 2025, [https://www.reddit.com/r/LocalLLaMA/comments/1ehlazq/introducing\_sqlitevec\_v010\_a\_vector\_search\_sqlite/](https://www.reddit.com/r/LocalLLaMA/comments/1ehlazq/introducing_sqlitevec_v010_a_vector_search_sqlite/)  
29. Looks really nice, but the only concern I had â€” how does the perf compare to mor... | Hacker News, accessed December 31, 2025, [https://news.ycombinator.com/item?id=40245266](https://news.ycombinator.com/item?id=40245266)
