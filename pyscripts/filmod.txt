Below is a complete implementation of the proposed module. The project is structured as follows:

folder_util/
├── __init__.py
├── config.py
├── folder_util.py         # Main entry point
├── cli.py                 # Command‑line argument parsing
├── scanner.py             # Directory scanning and metadata collection
├── filters.py             # Filtering functions
├── sorter.py              # Sorting utilities
├── display.py             # Rich‑based output formatting
├── reporter.py            # Exporting results (JSON, CSV, text)
└── utils
    ├── __init__.py
    ├── debug_utils.py   # Provided debug utilities
    ├── system_utils.py  # Provided system utilities
    └── path_utils.py    # Additional path helper(s)
setup.py
pyproject.toml
README.md

Below are the contents of each file.


---

folder_util/init.py

# folder_util/__init__.py

__version__ = "0.1.0"


---

folder_util/config.py

# folder_util/config.py

# Default configuration values for the folder_util module

DEFAULT_TARGET = "."
DEFAULT_RECURSIVE = False
DEFAULT_DEPTH = None          # No depth limit
DEFAULT_TRUNCATE = 20         # Maximum characters for names
DEFAULT_SORT = "date"         # Sort by date (newest first) by default
DEFAULT_OUTPUT_FORMAT = "table"  # "table", "json", or "csv"
DEFAULT_INCLUDE_SIZE = True
DEFAULT_INCLUDE_DATE = True
DEFAULT_INCLUDE_HIDDEN = False


---

folder_util/cli.py

# folder_util/cli.py

import argparse

def get_args():
    parser = argparse.ArgumentParser(
        description="Folder and File Utilities Tool"
    )
    # Basic options
    parser.add_argument("--target", "-t", type=str, default=".",
                        help="Target folder path.")
    parser.add_argument("--recursive", "-r", action="store_true",
                        help="Recursively scan subdirectories.")
    parser.add_argument("--depth", "-d", type=int, default=None,
                        help="Limit recursion to a specific depth.")
    parser.add_argument("--size", "-s", action="store_true",
                        help="Include file/folder sizes.")
    parser.add_argument("--date", "-D", action="store_true",
                        help="Include date added information.")
    parser.add_argument("--sort", "-S", type=str, default="date",
                        choices=["name", "size", "date"],
                        help="Sort criteria: name, size, or date.")
    parser.add_argument("--filter", "-f", type=str, default=None,
                        help="Regex pattern to filter file/folder names.")
    parser.add_argument("--output", "-o", type=str, default="table",
                        choices=["table", "json", "csv"],
                        help="Output format: table, json, or csv.")
    parser.add_argument("--truncate", "-c", type=int, default=20,
                        help="Maximum characters for name display.")
    parser.add_argument("--export", "-e", type=str, default=None,
                        help="File to export output (if specified, results will be written to this file).")
    
    # Extra columns options
    parser.add_argument("--permissions", "-p", action="store_true",
                        help="Include file/folder permissions column.")
    parser.add_argument("--date-modified", "-M", action="store_true",
                        help="Include last modified date/time column.")
    parser.add_argument("--date-created", "-C", action="store_true",
                        help="Include creation date/time column.")
    parser.add_argument("--git-repo", "-g", action="store_true",
                        help="Include a column indicating if the folder is a Git repository.")
    parser.add_argument("--git-status", "-G", action="store_true",
                        help="Include a column showing the Git status.")
    parser.add_argument("--owner", "-O", action="store_true",
                        help="Include the owner of the file/folder.")
    parser.add_argument("--file-count", "-n", action="store_true",
                        help="Include count of files within a folder.")
    parser.add_argument("--attributes", "-A", action="store_true",
                        help="Include extra file/folder attributes (hidden, read-only, etc.).")
    parser.add_argument("--date-accessed", "-X", action="store_true",
                        help="Include last accessed date/time column.")
    
    args = parser.parse_args()
    return args


---

folder_util/scanner.py

# folder_util/scanner.py

import os
import re
import stat
import datetime
from pathlib import Path
from typing import List, Dict, Any

from .utils.debug_utils import write_debug

def get_file_metadata(path: Path, include_size: bool, include_date: bool, extra: Dict[str, bool]) -> Dict[str, Any]:
    metadata = {}
    try:
        stat_result = path.stat()
    except Exception as e:
        write_debug(f"Error reading stat for {path}: {e}", channel="Warning")
        return metadata

    metadata['name'] = path.name
    metadata['full_path'] = str(path.resolve())
    if include_size and path.is_file():
        metadata['size'] = stat_result.st_size
    elif include_size and path.is_dir():
        metadata['size'] = calculate_folder_size(path)
    else:
        metadata['size'] = 0

    if include_date:
        # On Windows, st_ctime is the creation time; on Unix, try st_birthtime if available
        if os.name == 'nt':
            metadata['date_created'] = datetime.datetime.fromtimestamp(stat_result.st_ctime)
        else:
            metadata['date_created'] = datetime.datetime.fromtimestamp(getattr(stat_result, 'st_birthtime', stat_result.st_ctime))
        metadata['date_modified'] = datetime.datetime.fromtimestamp(stat_result.st_mtime)
        metadata['date_accessed'] = datetime.datetime.fromtimestamp(stat_result.st_atime)

    if extra.get("permissions", False):
        metadata['permissions'] = stat.filemode(stat_result.st_mode)
    if extra.get("owner", False):
        try:
            import pwd
            metadata['owner'] = pwd.getpwuid(stat_result.st_uid).pw_name
        except Exception:
            metadata['owner'] = "N/A"
    # Additional metadata (e.g., git info, file_count) can be added later.
    return metadata

def calculate_folder_size(folder: Path) -> int:
    total_size = 0
    try:
        for item in folder.rglob('*'):
            if item.is_file():
                try:
                    total_size += item.stat().st_size
                except Exception:
                    continue
    except Exception as e:
        write_debug(f"Error calculating folder size for {folder}: {e}", channel="Warning")
    return total_size

def scan_directory(target: str, recursive: bool, depth: int, include_hidden: bool,
                   include_size: bool, include_date: bool, extra: dict, filter_pattern: str = None) -> List[Dict[str, Any]]:
    results = []
    target_path = Path(target)
    if not target_path.exists():
        write_debug(f"Target path {target} does not exist.", channel="Error")
        return results

    def scan(path: Path, current_depth: int):
        if depth is not None and current_depth > depth:
            return
        try:
            for entry in path.iterdir():
                if not include_hidden and entry.name.startswith('.'):
                    continue
                if filter_pattern and not re.search(filter_pattern, entry.name):
                    continue
                metadata = get_file_metadata(entry, include_size, include_date, extra)
                metadata['is_dir'] = entry.is_dir()
                results.append(metadata)
                if recursive and entry.is_dir():
                    scan(entry, current_depth + 1)
        except Exception as e:
            write_debug(f"Error scanning {path}: {e}", channel="Warning")
    scan(target_path, 0)
    return results


---

folder_util/filters.py

# folder_util/filters.py

import re
import datetime
from typing import List, Dict, Any

def filter_by_size(items: List[Dict[str, Any]], min_size: int = None, max_size: int = None) -> List[Dict[str, Any]]:
    filtered = []
    for item in items:
        size = item.get('size', 0)
        if min_size is not None and size < min_size:
            continue
        if max_size is not None and size > max_size:
            continue
        filtered.append(item)
    return filtered

def filter_by_date(items: List[Dict[str, Any]], start_date: datetime.datetime = None, end_date: datetime.datetime = None,
                   date_field: str = "date_created") -> List[Dict[str, Any]]:
    filtered = []
    for item in items:
        date_val = item.get(date_field, None)
        if date_val is None:
            continue
        if start_date and date_val < start_date:
            continue
        if end_date and date_val > end_date:
            continue
        filtered.append(item)
    return filtered

def filter_by_regex(items: List[Dict[str, Any]], pattern: str, field: str = "name") -> List[Dict[str, Any]]:
    regex = re.compile(pattern)
    return [item for item in items if regex.search(item.get(field, ""))]


---

folder_util/sorter.py

# folder_util/sorter.py

def sort_items(items, sort_key="date", reverse=True):
    if sort_key == "name":
        return sorted(items, key=lambda x: x.get("name", "").lower(), reverse=reverse)
    elif sort_key == "size":
        return sorted(items, key=lambda x: x.get("size", 0), reverse=reverse)
    elif sort_key == "date":
        return sorted(items, key=lambda x: x.get("date_created"), reverse=reverse)
    else:
        return sorted(items, key=lambda x: x.get("name", "").lower(), reverse=reverse)


---

folder_util/display.py

# folder_util/display.py

from rich.table import Table
from rich.console import Console
from rich import box

def display_items(items, columns, truncate=20, sort_key="date"):
    console = Console()
    table = Table(show_header=True, header_style="bold magenta", box=box.MINIMAL_DOUBLE_HEAD)
    
    # Always include the Name column
    table.add_column("Name", style="cyan", no_wrap=True)
    
    # Add extra columns based on flags
    for col in columns:
        if col == "size":
            table.add_column("Size (bytes)", style="green")
        elif col == "date_created":
            table.add_column("Date Created", style="yellow")
        elif col == "date_modified":
            table.add_column("Date Modified", style="yellow")
        elif col == "date_accessed":
            table.add_column("Date Accessed", style="yellow")
        elif col == "permissions":
            table.add_column("Permissions", style="blue")
        elif col == "owner":
            table.add_column("Owner", style="magenta")
        elif col == "file_count":
            table.add_column("File Count", style="green")
        elif col == "attributes":
            table.add_column("Attributes", style="cyan")
        elif col == "git_repo":
            table.add_column("Git Repo", style="red")
        elif col == "git_status":
            table.add_column("Git Status", style="red")
        else:
            table.add_column(col)
    
    for item in items:
        # Truncate the name if necessary
        name = item.get("name", "")
        if len(name) > truncate:
            name = name[:truncate] + "..."
        row = [name]
        for col in columns:
            if col == "size":
                value = str(item.get("size", ""))
            elif col in ["date_created", "date_modified", "date_accessed"]:
                dt = item.get(col)
                value = dt.strftime("%Y-%m-%d %H:%M") if dt else ""
            elif col == "permissions":
                value = item.get("permissions", "")
            elif col == "owner":
                value = item.get("owner", "")
            elif col == "file_count":
                value = str(item.get("file_count", ""))
            elif col == "attributes":
                value = item.get("attributes", "")
            elif col == "git_repo":
                value = item.get("git_repo", "")
            elif col == "git_status":
                value = item.get("git_status", "")
            else:
                value = str(item.get(col, ""))
            row.append(value)
        table.add_row(*row)
    console.print(table)


---

folder_util/reporter.py

# folder_util/reporter.py

import json
import csv

def export_json(items, filename):
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(items, f, default=str, indent=4)

def export_csv(items, filename, columns):
    with open(filename, "w", newline='', encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        header = ["Name"] + [col.capitalize().replace("_", " ") for col in columns]
        writer.writerow(header)
        for item in items:
            row = [item.get("name", "")]
            for col in columns:
                if col in ["date_created", "date_modified", "date_accessed"]:
                    dt = item.get(col)
                    value = dt.strftime("%Y-%m-%d %H:%M") if dt else ""
                else:
                    value = item.get(col, "")
                row.append(value)
            writer.writerow(row)

def export_text(items, filename, columns, truncate=20):
    with open(filename, "w", encoding="utf-8") as f:
        header = ["Name"] + [col.capitalize().replace("_", " ") for col in columns]
        col_widths = [max(len(h), truncate) for h in header]
        header_line = " | ".join(h.ljust(w) for h, w in zip(header, col_widths))
        f.write(header_line + "\n")
        f.write("-" * len(header_line) + "\n")
        for item in items:
            name = item.get("name", "")
            if len(name) > truncate:
                name = name[:truncate] + "..."
            row = [name]
            for col in columns:
                if col in ["date_created", "date_modified", "date_accessed"]:
                    dt = item.get(col)
                    value = dt.strftime("%Y-%m-%d %H:%M") if dt else ""
                else:
                    value = str(item.get(col, ""))
                row.append(value)
            line = " | ".join(str(val).ljust(w) for val, w in zip(row, col_widths))
            f.write(line + "\n")


---

folder_util/utils/path_utils.py

# folder_util/utils/path_utils.py

from pathlib import Path

def is_hidden(filepath: str) -> bool:
    p = Path(filepath)
    return p.name.startswith(".")


---

folder_util/utils/init.py

# folder_util/utils/__init__.py

# This file can be empty. It simply marks the utils directory as a package.


---

folder_util/utils/debug_utils.py

# folder_util/utils/debug_utils.py

import inspect
import sys
import platform
import os
import datetime

# --- Configuration ---
DEFAULT_CONSOLE_VERBOSITY = "Debug"
DEFAULT_LOG_VERBOSITY = "Warning"  # Log only more important messages by default
DEFAULT_LOG_DIR = os.path.expanduser("~/logs")
MAX_LOG_FILE_SIZE_KB = 128         # Max size per log file in KB
MAX_TOTAL_LOG_SIZE_MB = 512        # Max total size for all logs in MB
MAX_LOG_FILES = 20                 # Maximum number of log files to keep per directory
LOG_FILE_EXTENSION = ".log"

# --- Global Variables ---
_console_verbosity_level = DEFAULT_CONSOLE_VERBOSITY
_log_verbosity_level = DEFAULT_LOG_VERBOSITY
_log_dir = DEFAULT_LOG_DIR
_log_file_enabled = False          # Indicates if file logging is enabled
_current_log_filepath = None       # Path to the currently active log file

def set_console_verbosity(level: str = DEFAULT_CONSOLE_VERBOSITY) -> None:
    global _console_verbosity_level
    _console_verbosity_level = _validate_verbosity_level(level, "console")

def set_log_verbosity(level: str = DEFAULT_LOG_VERBOSITY) -> None:
    global _log_verbosity_level
    _log_verbosity_level = _validate_verbosity_level(level, "log")

def set_log_directory(filepath: str = DEFAULT_LOG_DIR) -> None:
    global _log_dir
    _log_dir = os.path.expanduser(filepath)

def enable_file_logging() -> None:
    global _log_file_enabled, _current_log_filepath
    print("[Debug] Enabling file logging...")
    _log_file_enabled = True
    _current_log_filepath = _initialize_log_file()
    print(f"[Debug] Current log file: {_current_log_filepath}")

def disable_file_logging() -> None:
    global _log_file_enabled, _current_log_filepath
    _log_file_enabled = False
    _current_log_filepath = None

def _validate_verbosity_level(level: str, target_type: str) -> str:
    verbosity_levels = ["Verbose", "Debug", "Information", "Warning", "Error", "Critical"]
    level_capitalized = level.capitalize()
    if level_capitalized not in verbosity_levels:
        raise ValueError(f"Invalid {target_type} verbosity level: '{level}'. Must be one of {verbosity_levels}")
    return level_capitalized

def _is_at_verbosity_level(channel: str, verbosity_level: str) -> bool:
    verbosity_levels = ["Verbose", "Debug", "Information", "Warning", "Error", "Critical"]
    current_index = verbosity_levels.index(verbosity_level)
    channel_index = verbosity_levels.index(channel.capitalize())
    return channel_index >= current_index

def _get_log_filename_prefix() -> str:
    try:
        import git
        repo = git.Repo('.', search_parent_directories=True)
        repo_name = os.path.basename(repo.working_dir)
        return repo_name
    except Exception:
        return "cross_platform_log"

def _initialize_log_file() -> str:
    log_prefix = _get_log_filename_prefix()
    caller_filename = os.path.splitext(os.path.basename(inspect.stack()[2].filename))[0]
    date_str = datetime.date.today().isoformat()

    log_dir = os.path.join(_log_dir, log_prefix or caller_filename)
    log_filename = f"{date_str}{LOG_FILE_EXTENSION}" if log_prefix else f"{caller_filename}_{date_str}{LOG_FILE_EXTENSION}"

    os.makedirs(log_dir, exist_ok=True)
    log_filepath = os.path.join(log_dir, log_filename)

    if os.path.exists(log_filepath) and os.path.getsize(log_filepath) > MAX_LOG_FILE_SIZE_KB * 1024:
        _rotate_logs(log_dir, log_filename)

    _cleanup_old_logs(log_dir)
    return log_filepath

def _rotate_logs(log_dir: str, current_log_filename: str) -> None:
    base, ext = os.path.splitext(current_log_filename)
    timestamp = datetime.datetime.now().strftime("%H%M%S")
    rotated_filename = f"{base}_{timestamp}{ext}"
    current_filepath = os.path.join(log_dir, current_log_filename)
    rotated_filepath = os.path.join(log_dir, rotated_filename)
    try:
        if os.path.exists(current_filepath):
            os.rename(current_filepath, rotated_filepath)
    except Exception as e:
        print(f"[Warning] Log rotation failed: {e}")

def _cleanup_old_logs(log_dir: str) -> None:
    log_files = sorted(
        [entry for entry in os.scandir(log_dir) if entry.is_file() and entry.name.endswith(LOG_FILE_EXTENSION)],
        key=lambda entry: entry.stat().st_mtime
    )
    total_size = sum(entry.stat().st_size for entry in log_files)
    max_total_bytes = MAX_TOTAL_LOG_SIZE_MB * 1024 * 1024

    files_to_delete = []
    while total_size > max_total_bytes and log_files:
        oldest = log_files.pop(0)
        files_to_delete.append(oldest)
        total_size -= oldest.stat().st_size

    if len(log_files) + len(files_to_delete) > MAX_LOG_FILES:
        excess = (len(log_files) + len(files_to_delete)) - MAX_LOG_FILES
        files_to_delete.extend(log_files[:excess])

    for entry in files_to_delete:
        try:
            os.remove(entry.path)
            print(f"[Debug] Deleted old log file: {entry.name}")
        except Exception as e:
            print(f"[Warning] Failed to delete old log file {entry.name}: {e}")

def write_debug(message: str = "", channel: str = "Debug", condition: bool = True,
                output_stream: str = "stdout", location_channels=None) -> None:
    if not condition:
        return

    channel_cap = channel.capitalize()
    global _current_log_filepath

    output_message = message

    show_location = False
    if isinstance(location_channels, bool):
        show_location = location_channels
    elif isinstance(location_channels, list):
        show_location = channel_cap in location_channels

    if show_location:
        caller = inspect.stack()[1]
        output_message = f"[{caller.filename}:{caller.lineno}] {message}"

    color_map = {
        "Error": "\033[91m", "Warning": "\033[93m", "Verbose": "\033[90m",
        "Information": "\033[96m", "Debug": "\033[92m", "Critical": "\033[95m"
    }
    reset_color = "\033[0m"
    supports_color = sys.stdout.isatty() and platform.system() != "Windows"
    color = color_map.get(channel_cap, "") if supports_color else ""
    formatted_message = f"{color}[{channel_cap}]{reset_color} {output_message}" if color else f"[{channel_cap}] {output_message}"

    stream = sys.stdout if output_stream.lower() == "stdout" else sys.stderr
    if _is_at_verbosity_level(channel_cap, _console_verbosity_level):
        print(formatted_message, file=stream)

    if _log_file_enabled and _is_at_verbosity_level(channel_cap, _log_verbosity_level):
        if not _current_log_filepath:
            _current_log_filepath = _initialize_log_file()
        try:
            with open(_current_log_filepath, "a") as log_file:
                log_file.write(f"[{channel_cap}] {output_message}\n")
        except Exception as e:
            print(f"[Error] Failed to write to log file {_current_log_filepath}: {e}")


---

folder_util/utils/system_utils.py

# folder_util/utils/system_utils.py

import platform
import subprocess
import os
from .debug_utils import write_debug

class SystemUtils:
    """Base class for cross-platform system utilities."""
    def __init__(self):
        self.os_name = platform.system().lower()
        write_debug(f"Initialized SystemUtils for OS: {self.os_name}", channel="Debug")
    def is_termux(self) -> bool:
        is_termux = "ANDROID_ROOT" in os.environ and "com.termux" in os.environ.get("SHELL", "")
        write_debug(f"is_termux: {is_termux}", channel="Debug")
        return is_termux
    def is_wsl2(self) -> bool:
        is_wsl = "microsoft" in platform.uname().release.lower()
        write_debug(f"is_wsl2: {is_wsl}", channel="Debug")
        return is_wsl
    def run_command(self, command: str, sudo: bool = False) -> str:
        try:
            if sudo and self.os_name in ["linux", "darwin"] and not self.is_termux():
                command = f"sudo {command}"
                write_debug(f"Prepended sudo: {command}", channel="Debug")
            write_debug(f"Running command: {command}", channel="Debug")
            result = subprocess.run(command, shell=True, text=True, capture_output=True)
            if result.returncode == 0:
                write_debug(f"Command succeeded: {command}", channel="Debug")
                return result.stdout.strip()
            else:
                write_debug(f"Command failed: {command}\nError: {result.stderr.strip()}", channel="Error")
                return ""
        except Exception as e:
            write_debug(f"Exception while running command '{command}': {e}", channel="Critical")
            return ""
    def source_file(self, filepath: str) -> bool:
        try:
            if self.os_name in ["linux", "darwin"] and not self.is_termux():
                command = f"source {filepath}"
                write_debug(f"Attempting to source file with command: {command}", channel="Debug")
                result = subprocess.run(["zsh", "-c", command], text=True, capture_output=True)
                if result.returncode == 0:
                    write_debug(f"Sourced file successfully: {filepath}", channel="Debug")
                    return True
                else:
                    write_debug(f"Failed to source file: {filepath}\nError: {result.stderr.strip()}", channel="Error")
                    return False
            else:
                write_debug(f"Automatic sourcing not supported on OS: {self.os_name} or under Termux.", channel="Debug")
                return False
        except Exception as e:
            write_debug(f"Exception while sourcing file {filepath}: {e}", channel="Critical")
            return False


---

folder_util/folder_util.py (Main Entry Point)

# folder_util/folder_util.py

import sys
from .cli import get_args
from .scanner import scan_directory
from .sorter import sort_items
from .display import display_items
from .reporter import export_json, export_csv, export_text
from .filters import filter_by_size, filter_by_date, filter_by_regex
from .utils.debug_utils import write_debug

def main():
    args = get_args()
    write_debug("Parsed command line arguments.", channel="Debug")
    
    # Determine which extra columns to include based on CLI flags
    extra_columns = []
    if args.size:
        extra_columns.append("size")
    if args.date:
        extra_columns.append("date_created")
    if args.date_modified:
        extra_columns.append("date_modified")
    if args.date_created:
        extra_columns.append("date_created")
    if args.date_accessed:
        extra_columns.append("date_accessed")
    if args.permissions:
        extra_columns.append("permissions")
    if args.owner:
        extra_columns.append("owner")
    if args.file_count:
        extra_columns.append("file_count")
    if args.attributes:
        extra_columns.append("attributes")
    if args.git_repo:
        extra_columns.append("git_repo")
    if args.git_status:
        extra_columns.append("git_status")
    
    # Extra flags passed to the scanner (for metadata collection)
    extra = {
        "permissions": args.permissions,
        "owner": args.owner,
        # Additional extra metadata flags can be added here.
    }
    
    # Scan the target directory
    items = scan_directory(
        target=args.target,
        recursive=args.recursive,
        depth=args.depth,
        include_hidden=False,  # Could add a CLI flag for hidden files if desired
        include_size=args.size,
        include_date=args.date or args.date_created or args.date_modified or args.date_accessed,
        extra=extra,
        filter_pattern=args.filter
    )
    
    write_debug(f"Scanned {len(items)} items.", channel="Debug")
    
    # Sort items based on the selected criteria
    sorted_items = sort_items(items, sort_key=args.sort)
    
    # Display or output the results
    if args.output.lower() == "table":
        display_items(sorted_items, columns=extra_columns, truncate=args.truncate, sort_key=args.sort)
    elif args.output.lower() == "json":
        import json
        print(json.dumps(sorted_items, default=str, indent=4))
    elif args.output.lower() == "csv":
        import csv
        writer = csv.writer(sys.stdout)
        header = ["Name"] + [col.capitalize() for col in extra_columns]
        writer.writerow(header)
        for item in sorted_items:
            row = [item.get("name", "")]
            for col in extra_columns:
                value = item.get(col, "")
                if hasattr(value, "strftime"):
                    value = value.strftime("%Y-%m-%d %H:%M")
                row.append(value)
            writer.writerow(row)
    
    # Export results if an export file is specified
    if args.export:
        if args.output.lower() == "json":
            export_json(sorted_items, args.export)
        elif args.output.lower() == "csv":
            export_csv(sorted_items, args.export, extra_columns)
        elif args.output.lower() == "table":
            export_text(sorted_items, args.export, extra_columns, truncate=args.truncate)
        write_debug(f"Exported results to {args.export}", channel="Information")

if __name__ == "__main__":
    main()


---

setup.py

# setup.py

from setuptools import setup, find_packages

setup(
    name="folder_util",
    version="0.1.0",
    description="A comprehensive folder and file utilities tool with rich CLI output",
    author="Your Name",
    author_email="your.email@example.com",
    packages=find_packages(),
    install_requires=[
        "rich>=10.0.0",
        # Optionally add other requirements, e.g., GitPython if needed.
    ],
    entry_points={
        "console_scripts": [
            "folder_util=folder_util.folder_util:main"
        ]
    },
    classifiers=[
        "Programming Language :: Python :: 3",
        "Operating System :: OS Independent",
    ],
)


---

pyproject.toml

# pyproject.toml

[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "folder_util"
version = "0.1.0"
description = "A comprehensive folder and file utilities tool with rich CLI output"
authors = [
    { name="Your Name", email="your.email@example.com" }
]
dependencies = [
    "rich>=10.0.0",
]


---

README.md

# Folder Util

A comprehensive folder and file utilities tool that supports a rich command‑line interface,
filtering, sorting, and export options.

## Installation

Clone the repository and install via pip:

```bash
git clone https://your.repo.url/folder_util.git
cd folder_util
pip install .

Usage

Run the tool from the command line:

folder_util --target /path/to/folder --size -s --date -D --permissions -p --sort date --export output.txt

Use -h or --help for a complete list of options.

---

This complete set of files provides the requested module with a rich set of features, modular design, and command‑line arguments (with both long and one‑letter abbreviated options). The code uses the [rich](https://rich.readthedocs.io/) library for beautiful output and integrates your provided cross‑platform debug and system utilities. 

You can now install this package (via pip or setup.py) and run it from the command line as described in the README.
